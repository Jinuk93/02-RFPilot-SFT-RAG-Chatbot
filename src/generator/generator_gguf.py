from llama_cpp import Llama
from typing import Optional, Dict, Any, List
import logging
import time
import os

from src.utils.config import RAGConfig
from src.router.query_router import QueryRouter
from src.prompts.dynamic_prompts import PromptManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GGUFGenerator:
    """
    GGUF ê¸°ë°˜ Llama-3 ìƒì„±ê¸°
    
    llama.cppë¥¼ ì‚¬ìš©í•˜ì—¬ GGUF í¬ë§· ëª¨ë¸ì„ ë¡œë“œí•˜ê³ 
    ì…ì°° ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        model_path: str,
        n_gpu_layers: int = 0,
        n_ctx: int = 2048,
        n_threads: int = 8,
        config = None,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        system_prompt: str = "ë‹¹ì‹ ì€ RFP(ì œì•ˆìš”ì²­ì„œ) ë¶„ì„ ë° ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
    ):
        """
        ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path: GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            n_gpu_layers: GPUì— ì˜¬ë¦´ ë ˆì´ì–´ ìˆ˜ (0 = CPUë§Œ, 35 = ì „ì²´ GPU)
            n_ctx: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads: CPU ìŠ¤ë ˆë“œ ìˆ˜
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ë‹¤ì–‘ì„± (0.0~1.0)
            top_p: Nucleus sampling íŒŒë¼ë¯¸í„°
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        """
        self.config = config or RAGConfig() 
        self.model_path = model_path
        self.n_gpu_layers = n_gpu_layers
        self.n_ctx = n_ctx
        self.n_threads = n_threads
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.system_prompt = system_prompt
        
        # ëª¨ë¸ (ë‚˜ì¤‘ì— ë¡œë“œ)
        self.model = None
        
        logger.info(f"GGUFGenerator ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_model(self) -> None:
        """
        GGUF ëª¨ë¸ ë¡œë“œ
        
        ë¡œì§:
        1. USE_MODEL_HUB í™•ì¸
        2-A. True â†’ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ
        2-B. False â†’ ë¡œì»¬ íŒŒì¼ ì‚¬ìš©
        3. ëª¨ë¸ ë¡œë“œ
        """
        
        # ì¤‘ë³µ ë¡œë“œ ë°©ì§€
        if self.model is not None:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        try:
            # Configì—ì„œ USE_MODEL_HUB í™•ì¸ (ì—†ìœ¼ë©´ True ê¸°ë³¸ê°’)
            use_model_hub = getattr(self.config, 'USE_MODEL_HUB', True)
            
            # Model Hub ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ê²½ë¡œ ê²°ì •
            if use_model_hub:
                # === Model Hubì—ì„œ ë‹¤ìš´ë¡œë“œ ===
                model_hub_repo = getattr(self.config, 'MODEL_HUB_REPO', 'beomi/Llama-3-Open-Ko-8B-gguf')
                model_hub_filename = getattr(self.config, 'MODEL_HUB_FILENAME', 'ggml-model-Q4_K_M.gguf')
                model_cache_dir = getattr(self.config, 'MODEL_CACHE_DIR', '.cache/models')
                
                logger.info(f"ğŸ“¥ Model Hubì—ì„œ ë‹¤ìš´ë¡œë“œ: {model_hub_repo}")
                
                from huggingface_hub import hf_hub_download
                
                model_path = hf_hub_download(
                    repo_id=model_hub_repo,
                    filename=model_hub_filename,
                    cache_dir=model_cache_dir,
                    local_dir=model_cache_dir,
                    local_dir_use_symlinks=False  # ì‹¬ë³¼ë¦­ ë§í¬ ëŒ€ì‹  ì‹¤ì œ ë³µì‚¬
                )
                
                logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
                
            else:
                # === ë¡œì»¬ íŒŒì¼ ì‚¬ìš© ===
                model_path = self.model_path  # ìƒì„±ìì—ì„œ ë°›ì€ ê²½ë¡œ ì‚¬ìš©
                
                if not os.path.exists(model_path):
                    raise FileNotFoundError(
                        f"âŒ ë¡œì»¬ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\n"
                        f"   USE_MODEL_HUB=trueë¡œ ì„¤ì •í•˜ê±°ë‚˜ ëª¨ë¸ íŒŒì¼ì„ ì¤€ë¹„í•˜ì„¸ìš”."
                    )
                
                logger.info(f"ğŸ“‚ ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©: {model_path}")
            
            # === ê³µí†µ: ëª¨ë¸ ë¡œë“œ ===
            logger.info(f"ğŸš€ GGUF ëª¨ë¸ ë¡œë“œ ì¤‘...")
            logger.info(f"   GPU ë ˆì´ì–´: {self.n_gpu_layers}")
            logger.info(f"   ì»¨í…ìŠ¤íŠ¸: {self.n_ctx}")
            
            self.model = Llama(
                model_path=model_path,
                n_gpu_layers=self.n_gpu_layers,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                verbose=False,
            )
            
            logger.info("âœ… GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            
        except FileNotFoundError as e:
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def format_prompt(
        self,
        question: str,
        context: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """
        GGUF ëª¨ë¸ìš© ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        
        Llama-3 íŠ¹ìˆ˜ í† í° ëŒ€ì‹  ìˆœìˆ˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ í…œí”Œë¦¿ ì‚¬ìš©
        """
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        if system_prompt is None:
            system_prompt = self.system_prompt
        
        # ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
        if context is not None:
            user_message = f"ì°¸ê³  ë¬¸ì„œ:\n{context}\n\nì§ˆë¬¸: {question}"
        else:
            user_message = question
        
        # ê°„ë‹¨í•œ í•œêµ­ì–´ í…œí”Œë¦¿ (íŠ¹ìˆ˜ í† í° ì—†ìŒ)
        formatted_prompt = f"""### ì‹œìŠ¤í…œ
{system_prompt}

### ì‚¬ìš©ì
{user_message}

### ë‹µë³€
"""
        
        return formatted_prompt
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
    ) -> str:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì‘ë‹µ ìƒì„±
        
        Args:
            prompt: í¬ë§·ëœ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ë‹¤ì–‘ì„±
            top_p: Nucleus sampling
        
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        
        Raises:
            RuntimeError: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        """
        # ëª¨ë¸ ë¡œë“œ í™•ì¸
        if self.model is None:
            raise RuntimeError(
                "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )
        
        # íŒŒë¼ë¯¸í„° ì„¤ì •
        if max_new_tokens is None:
            max_new_tokens = self.max_new_tokens
        if temperature is None:
            temperature = self.temperature
        if top_p is None:
            top_p = self.top_p
        
        try:
            logger.info(f"ğŸ”„ ìƒì„± ì‹œì‘ (max_tokens={max_new_tokens}, temp={temperature})")
            start_time = time.time()
            
            # ìƒì„±
            output = self.model(
                prompt,
                max_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                echo=False,  # í”„ë¡¬í”„íŠ¸ ë°˜ë³µ ì•ˆ í•¨
                stop=[
                    "###", "\n\n###", 
                    "### ì‚¬ìš©ì", "\nì‚¬ìš©ì:", 
                    "</s>",
                    "í•œêµ­ì–´ ë‹µë³€", "í•œêµ­ì–´ë¡œ ë‹µë³€", "ì§€ì¹¨:",  # ë©”íƒ€ í…ìŠ¤íŠ¸ ì°¨ë‹¨
                    "ë¬¸ì¥", "(ë¬¸ì¥"  # ë©”íƒ€ ë²ˆí˜¸ ì°¨ë‹¨
                ],
            )
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            
            # ì‘ë‹µ ì¶”ì¶œ
            response = output['choices'][0]['text'].strip()
            
            logger.info(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response)} ê¸€ì")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise RuntimeError(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def chat(
        self,
        question: str,
        context: Optional[str] = None,
        system_prompt=None,
        **kwargs
    ) -> str:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (í†µí•© ë©”ì„œë“œ)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì„ íƒì  ì»¨í…ìŠ¤íŠ¸
            system_prompt: ì„ íƒì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            **kwargs: generate() ë©”ì„œë“œì— ì „ë‹¬ë  ì¶”ê°€ íŒŒë¼ë¯¸í„°
        
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        prompt = self.format_prompt(
            question=question,
            context=context,
            system_prompt=system_prompt
        )
        
        # ì‘ë‹µ ìƒì„±
        response = self.generate(prompt, **kwargs)
        
        return response


class GGUFRAGPipeline:
    """
    GGUF ìƒì„±ê¸° + RAG í†µí•© íŒŒì´í”„ë¼ì¸
    
    chatbot_app.pyì™€ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    """
    
    def __init__(
        self,
        config=None,
        model: str = None,  # í˜¸í™˜ì„±ìš© (ì‚¬ìš© ì•ˆ í•¨)
        top_k: int = None,
        # GPU ì„¤ì • (ì„ íƒì , config ì˜¤ë²„ë¼ì´ë“œ)
        n_gpu_layers: int = None,
        n_ctx: int = None,
        n_threads: int = None,
        max_new_tokens: int = None,
        temperature: float = None,
        top_p: float = None,
        search_mode: str = None,
        alpha: float = None
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: RAGConfig ê°ì²´
            model: ëª¨ë¸ ì´ë¦„ (ì‚¬ìš© ì•ˆ í•¨, í˜¸í™˜ì„±ìš©)
            top_k: ê¸°ë³¸ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
            n_gpu_layers: GPU ë ˆì´ì–´ ìˆ˜ (config ì˜¤ë²„ë¼ì´ë“œ)
            n_ctx: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (config ì˜¤ë²„ë¼ì´ë“œ)
            n_threads: CPU ìŠ¤ë ˆë“œ ìˆ˜ (config ì˜¤ë²„ë¼ì´ë“œ)
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° (config ì˜¤ë²„ë¼ì´ë“œ)
            temperature: ìƒì„± ë‹¤ì–‘ì„± (config ì˜¤ë²„ë¼ì´ë“œ)
            top_p: Nucleus sampling (config ì˜¤ë²„ë¼ì´ë“œ)
            search_mode: ê²€ìƒ‰ ëª¨ë“œ
            alpha: ì„ë² ë”© ê°€ì¤‘ì¹˜
        """
        self.config = config or RAGConfig()
        
        # Configì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ fallback)
        self.top_k = top_k or getattr(self.config, 'DEFAULT_TOP_K', 10)
        
        # ê²€ìƒ‰ ì„¤ì •
        self.search_mode = search_mode or getattr(self.config, 'DEFAULT_SEARCH_MODE', 'hybrid_rerank')
        self.alpha = alpha if alpha is not None else getattr(self.config, 'DEFAULT_ALPHA', 0.5)
        
        # Retriever ì´ˆê¸°í™” (RAGRetriever ì‚¬ìš©)
        logger.info("RAGRetriever ì´ˆê¸°í™” ì¤‘...")
        from src.retriever.retriever import RAGRetriever
        self.retriever = RAGRetriever(config=self.config)
        
        # GGUF ì„¤ì • (íŒŒë¼ë¯¸í„°ê°€ ì£¼ì–´ì§€ë©´ config ì˜¤ë²„ë¼ì´ë“œ, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
        gguf_n_gpu_layers = n_gpu_layers if n_gpu_layers is not None else getattr(self.config, 'GGUF_N_GPU_LAYERS', 35)
        gguf_n_ctx = n_ctx if n_ctx is not None else getattr(self.config, 'GGUF_N_CTX', 2048)
        gguf_n_threads = n_threads if n_threads is not None else getattr(self.config, 'GGUF_N_THREADS', 4)
        gguf_max_new_tokens = max_new_tokens if max_new_tokens is not None else getattr(self.config, 'GGUF_MAX_NEW_TOKENS', 512)
        gguf_temperature = temperature if temperature is not None else getattr(self.config, 'GGUF_TEMPERATURE', 0.7)
        gguf_top_p = top_p if top_p is not None else getattr(self.config, 'GGUF_TOP_P', 0.9)
        
        # ëª¨ë¸ ê²½ë¡œ (fallback)
        gguf_model_path = getattr(self.config, 'GGUF_MODEL_PATH', '.cache/models/llama-3-ko-8b.gguf')
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (fallback)
        system_prompt = getattr(self.config, 'SYSTEM_PROMPT', 'ë‹¹ì‹ ì€ í•œêµ­ ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.')
        
        # GGUFGenerator ì´ˆê¸°í™”
        logger.info("GGUFGenerator ì´ˆê¸°í™” ì¤‘...")
        logger.info(f"   GPU ë ˆì´ì–´: {gguf_n_gpu_layers}")
        logger.info(f"   ì»¨í…ìŠ¤íŠ¸: {gguf_n_ctx}")
        logger.info(f"   ìŠ¤ë ˆë“œ: {gguf_n_threads}")
        logger.info(f"   ëª¨ë¸ ê²½ë¡œ: {gguf_model_path}")
        
        self.generator = GGUFGenerator(
            model_path=gguf_model_path,
            n_gpu_layers=gguf_n_gpu_layers,
            n_ctx=gguf_n_ctx,
            n_threads=gguf_n_threads,
            config=self.config,
            max_new_tokens=gguf_max_new_tokens,
            temperature=gguf_temperature,
            top_p=gguf_top_p,
            system_prompt=system_prompt
        )
        
        # ëª¨ë¸ ë¡œë“œ (ì‹œê°„ ì†Œìš”)
        logger.info("GGUF ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.generator.load_model()
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.chat_history: List[Dict] = []
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ (sources ë°˜í™˜ìš©)
        self._last_retrieved_docs = []
        
        logger.info("âœ… GGUFRAGPipeline ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - ê²€ìƒ‰ ëª¨ë“œ: {self.search_mode}")
        logger.info(f"   - ê¸°ë³¸ top_k: {self.top_k}")
    
    def _retrieve_and_format(self, query: str) -> str:
        """ê²€ìƒ‰ ìˆ˜í–‰ ë° ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ë¬¸ì„œ ê²€ìƒ‰ (RAGRetriever ë©”ì„œë“œ ì‚¬ìš©)
        if self.search_mode == "embedding":
            docs = self.retriever.search(query, top_k=self.top_k)
        elif self.search_mode == "embedding_rerank":
            docs = self.retriever.search_with_rerank(query, top_k=self.top_k)
        elif self.search_mode == "hybrid":
            docs = self.retriever.hybrid_search(
                query, top_k=self.top_k, alpha=self.alpha
            )
        elif self.search_mode == "hybrid_rerank":
            docs = self.retriever.hybrid_search_with_rerank(
                query, top_k=self.top_k, alpha=self.alpha
            )
        else:
            docs = self.retriever.search(query, top_k=self.top_k)
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        self._last_retrieved_docs = docs
        
        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        return self._format_context(docs)
    
    def _format_context(self, retrieved_docs: list) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë™ìœ¼ë¡œ ì¤„ì„ (í† í° ì œí•œ ëŒ€ì‘)
        """
        if not retrieved_docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        max_context_chars = 8000  # ëŒ€ëµ 2000 í† í° ì •ë„ (ì—¬ìœ  ìˆê²Œ)
        
        current_length = 0
        for i, doc in enumerate(retrieved_docs, 1):
            doc_text = f"[ë¬¸ì„œ {i}]\n{doc['content']}\n"
            doc_length = len(doc_text)
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì²´í¬
            if current_length + doc_length > max_context_chars:
                logger.warning(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ: {i-1}ê°œ ë¬¸ì„œë§Œ ì‚¬ìš© (ìµœëŒ€ {max_context_chars}ì)")
                break
            
            context_parts.append(doc_text)
            current_length += doc_length
        
        return "\n".join(context_parts)
    
    def _format_sources(self, retrieved_docs: list) -> list:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        sources = []
        for doc in retrieved_docs:
            source_info = {
                'content': doc['content'],
                'metadata': doc['metadata'],
                'filename': doc.get('filename', 'N/A'),
                'organization': doc.get('organization', 'N/A')
            }
            
            # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ì ìˆ˜ í•„ë“œê°€ ë‹¤ë¦„
            if 'rerank_score' in doc:
                source_info['score'] = doc['rerank_score']
                source_info['score_type'] = 'rerank'
            elif 'hybrid_score' in doc:
                source_info['score'] = doc['hybrid_score']
                source_info['score_type'] = 'hybrid'
            elif 'relevance_score' in doc:
                source_info['score'] = doc['relevance_score']
                source_info['score_type'] = 'embedding'
            else:
                source_info['score'] = 0
                source_info['score_type'] = 'unknown'
            
            sources.append(source_info)
        
        return sources
    
    def _estimate_usage(self, query: str, answer: str) -> dict:
        """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        # ê°„ë‹¨í•œ ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ì¶”ì •
        prompt_tokens = len(query.split()) * 2
        completion_tokens = len(answer.split()) * 2
        
        return {
            'total_tokens': prompt_tokens + completion_tokens,
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens
        }
    
    def generate_answer(
        self,
        query: str,
        top_k: int = None,
        search_mode: str = None,
        alpha: float = None
    ) -> dict:
        """
        ë‹µë³€ ìƒì„± (chatbot_app.py í˜¸í™˜ ë©”ì¸ ë©”ì„œë“œ)
        
        Args:
            query: ì§ˆë¬¸
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            search_mode: ê²€ìƒ‰ ëª¨ë“œ
            alpha: ì„ë² ë”© ê°€ì¤‘ì¹˜
        
        Returns:
            dict: answer, sources, search_mode, usage, elapsed_time, used_retrieval
        """
        try:
            start_time = time.time()
            
            # íŒŒë¼ë¯¸í„° ì„¤ì • (ê²€ìƒ‰ ì „ì— ë¨¼ì € ì„¤ì •)
            if top_k is not None:
                self.top_k = top_k
            if search_mode is not None:
                self.search_mode = search_mode
            if alpha is not None:
                self.alpha = alpha

            # ===== Routerë¡œ ê²€ìƒ‰ ì—¬ë¶€ ê²°ì • =====
            router = QueryRouter()
            classification = router.classify(query)
            query_type = classification['type']  # 'greeting'/'thanks'/'document'/'out_of_scope'
            
            logger.info(f"ğŸ“ ë¶„ë¥˜: {query_type} "
                f"(ì‹ ë¢°ë„: {classification['confidence']:.2f})")
            
            # 2. íƒ€ì…ë³„ ì²˜ë¦¬
            if query_type in ['greeting', 'thanks', 'out_of_scope']:
                # ê²€ìƒ‰ ìŠ¤í‚µ
                context = None
                used_retrieval = False
                self._last_retrieved_docs = []
                
                # ë™ì  í”„ë¡¬í”„íŠ¸ ì„ íƒ (GGUFìš©)
                system_prompt = PromptManager.get_prompt(query_type, model_type="gguf")
                logger.info(f"â­ï¸ RAG ìŠ¤í‚µ: {query_type}")
            
            elif query_type == 'document':
                # RAG ìˆ˜í–‰
                context = self._retrieve_and_format(query)
                used_retrieval = True
                
                # ë™ì  í”„ë¡¬í”„íŠ¸ (GGUFìš©, context í¬í•¨)
                system_prompt = PromptManager.get_prompt('document', model_type="gguf")
                logger.info(f"ğŸ” RAG ìˆ˜í–‰: {len(self._last_retrieved_docs)}ê°œ ë¬¸ì„œ")
            
            # 3. ë‹µë³€ ìƒì„± (system_prompt ì „ë‹¬)
            answer = self.generator.chat(
                question=query,
                context=context,
                system_prompt=system_prompt
            )
            
            elapsed_time = time.time() - start_time
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.chat_history.append({"role": "user", "content": query})
            self.chat_history.append({"role": "assistant", "content": answer})
            
            # ê²°ê³¼ ë°˜í™˜ (RAGPipelineê³¼ ë™ì¼ í˜•ì‹)
            return {
                'answer': answer,
                'sources': self._format_sources(self._last_retrieved_docs),
                'used_retrieval': used_retrieval,
                'query_type': query_type,
                'search_mode': self.search_mode if used_retrieval else 'direct',
                'routing_info': classification,
                'elapsed_time': elapsed_time,
                'usage': self._estimate_usage(query, answer)
            }
        
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}") from e
    
    def chat(self, query: str) -> str:
        """ê°„ë‹¨í•œ ëŒ€í™” ì¸í„°í˜ì´ìŠ¤"""
        result = self.generate_answer(query)
        return result['answer']
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.chat_history = []
        logger.info("ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.chat_history.copy()
    
    def set_search_config(
        self,
        search_mode: str = None,
        top_k: int = None,
        alpha: float = None
    ):
        """ê²€ìƒ‰ ì„¤ì • ë³€ê²½"""
        if search_mode is not None:
            self.search_mode = search_mode
        if top_k is not None:
            self.top_k = top_k
        if alpha is not None:
            self.alpha = alpha
        
        logger.info(
            f"ğŸ”§ ê²€ìƒ‰ ì„¤ì • ë³€ê²½: mode={self.search_mode}, "
            f"top_k={self.top_k}, alpha={self.alpha}"
        )