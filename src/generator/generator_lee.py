from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import HumanMessage, AIMessage
from langsmith import traceable
import time
from typing import List, Dict

from src.utils.config import RAGConfig
from src.retriever.retriever import RAGRetriever
from src.router.query_router import QueryRouter


class RAGPipeline:
    """ëŒ€í™”í˜• RAG íŒŒì´í”„ë¼ì¸ - LangChain Chain ê¸°ë°˜"""

    def __init__(self, config: RAGConfig = None, model: str = None, top_k: int = None):
        """ì´ˆê¸°í™”"""
        self.config = config or RAGConfig()
        self.model = model or self.config.LLM_MODEL_NAME
        self.top_k = top_k or self.config.DEFAULT_TOP_K
        
        # ê²€ìƒ‰ ì„¤ì •
        self.search_mode = self.config.DEFAULT_SEARCH_MODE
        self.alpha = self.config.DEFAULT_ALPHA

        # LLM ì´ˆê¸°í™” (LangChain ChatOpenAI)
        self.llm = ChatOpenAI(
            model=self.model,
            openai_api_key=self.config.OPENAI_API_KEY,
            timeout=60.0,
            max_retries=3
        )

        # Retriever ë° ë¼ìš°í„° ì´ˆê¸°í™”
        self.retriever = RAGRetriever(config=self.config)
        self.router = QueryRouter()
        self._direct_responses = {
            'greeting': "ì•ˆë…•í•˜ì„¸ìš”! ê³µê³µì…ì°° RFP ê´€ë ¨ ê¶ê¸ˆí•œ ì‚¬í•­ì„ ì•Œë ¤ì£¼ì‹œë©´ ìë£Œë¥¼ ì°¾ì•„ ë“œë¦´ê²Œìš”.",
            'thanks': "ë„ì›€ì´ ë˜ì—ˆë‹¤ë‹ˆ ë‹¤í–‰ì…ë‹ˆë‹¤. ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”!",
            'out_of_scope': "í•´ë‹¹ ì§ˆë¬¸ì€ í˜„ì¬ ë³´ìœ í•œ ì…ì°°Â·ì‚¬ì—… ë¬¸ì„œì—ì„œ ë‹¤ë£¨ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ ì£¼ì„¸ìš”."
        }
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.chat_history: List[Dict] = []
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ (sources ë°˜í™˜ìš©)
        self._last_retrieved_docs = []

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ê³µê³µì…ì°° RFPë¥¼ ë¶„ì„í•˜ëŠ” ì…ì°°ë©”ì´íŠ¸ ì‚¬ë‚´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ìš”êµ¬ì‚¬í•­Â·ì˜ˆì‚°Â·ëŒ€ìƒ ê¸°ê´€Â·ì œì¶œ ë°©ì‹ ë“±ì„ êµ¬ì¡°í™”í•´ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•˜ì„¸ìš”.

            # ê·œì¹™
            - ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
            - ì»¨í…ìŠ¤íŠ¸ ë°– ë‚´ìš©ì„ ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            - ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì‚¬ì‹¤ì´ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." í•œ ë¬¸ì¥ìœ¼ë¡œë§Œ ë‹µí•©ë‹ˆë‹¤.
            - ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë¹„êµí•  ë•ŒëŠ” ë¬¸ì„œë³„ ì°¨ì´ë¥¼ í‘œ ë˜ëŠ” ëª©ë¡ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
            - ìˆ«ìì—ëŠ” ê°€ëŠ¥í•œ ë‹¨ìœ„ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
            - ì§ì „ ëŒ€í™” ë§¥ë½ì„ ë°˜ì˜í•˜ë˜, í™•ì¸ë˜ì§€ ì•Šì€ ë‚´ìš©ì„ ì¶”ë¡ í•´ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

            # ë‹µë³€ í˜•ì‹
            1. í•œ ì¤„ ìš”ì•½: ì§ˆë¬¸ í•µì‹¬ì„ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
            2. ìƒì„¸ ë‹µë³€: [ìš”êµ¬ì‚¬í•­], [ëŒ€ìƒ ê¸°ê´€], [ì˜ˆì‚°], [ì œì¶œ í˜•ì‹/ë°©ë²•], [í‰ê°€ ê¸°ì¤€] ë“± ë¬¸ì„œì—ì„œ í™•ì¸ëœ í•­ëª©ë§Œ ì •ë¦¬í•©ë‹ˆë‹¤.
            3. ê·¼ê±° ì •ë³´: ìœ„ ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ì¥ì´ë‚˜ ë¬¸ë‹¨ì„ ìš”ì•½í•©ë‹ˆë‹¤.
            4. ë¶€ì¡±í•œ ì •ë³´: ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” í•­ëª©ì€ "ë¬¸ì„œì—ì„œ í™•ì¸ ë¶ˆê°€"ë¡œ í‘œê¸°í•©ë‹ˆë‹¤."""),
                        
                        # ëŒ€í™” íˆìŠ¤í† ë¦¬
                        MessagesPlaceholder(variable_name="chat_history"),
                        
                        # í˜„ì¬ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸
                        ("user", """# ì»¨í…ìŠ¤íŠ¸
            {context}

            # ì§ˆë¬¸
            {question}

            ìœ„ ê·œì¹™ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.""")
        ])

        # Chain êµ¬ì„±
        self.chain = (
            {
                "context": RunnableLambda(self._retrieve_and_format),
                "question": RunnablePassthrough(),
                "chat_history": RunnableLambda(lambda x: self._get_chat_history())
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

        print(f"âœ… RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ëª¨ë¸: {self.model}")
        print(f"   - ê¸°ë³¸ top_k: {self.top_k}")
        print(f"   - ê²€ìƒ‰ ëª¨ë“œ: {self.search_mode}")

    def _get_chat_history(self) -> List:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        messages = []
        for msg in self.chat_history:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            else:
                messages.append(AIMessage(content=msg["content"]))
        return messages

    def _retrieve_and_format(self, query: str) -> str:
        """ê²€ìƒ‰ ìˆ˜í–‰ ë° ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ë¬¸ì„œ ê²€ìƒ‰
        if self.search_mode == "embedding":
            docs = self.retriever.search(query, top_k=self.top_k)
        elif self.search_mode == "hybrid":
            docs = self.retriever.hybrid_search(query, top_k=self.top_k, alpha=self.alpha)
        elif self.search_mode == "hybrid_rerank":
            docs = self.retriever.hybrid_search_with_rerank(
                query, top_k=self.top_k, alpha=self.alpha
            )
        else:
            docs = self.retriever.search(query, top_k=self.top_k)
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        self._last_retrieved_docs = docs
        
        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        return self._format_context(docs)

    def _format_context(self, retrieved_docs: list) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not retrieved_docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, doc in enumerate(retrieved_docs, 1):
            context_parts.append(f"[ë¬¸ì„œ {i}]\n{doc['content']}\n")
        return "\n".join(context_parts)

    def _format_sources(self, retrieved_docs: list) -> list:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        sources = []
        for doc in retrieved_docs:
            source_info = {
                'content': doc['content'],
                'metadata': doc['metadata'],
                'filename': doc.get('filename', 'N/A'),
                'organization': doc.get('organization', 'N/A')
            }
            
            # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ì ìˆ˜ í•„ë“œê°€ ë‹¤ë¦„
            if 'rerank_score' in doc:
                source_info['score'] = doc['rerank_score']
                source_info['score_type'] = 'rerank'
            elif 'hybrid_score' in doc:
                source_info['score'] = doc['hybrid_score']
                source_info['score_type'] = 'hybrid'
            elif 'relevance_score' in doc:
                source_info['score'] = doc['relevance_score']
                source_info['score_type'] = 'embedding'
            else:
                source_info['score'] = 0
                source_info['score_type'] = 'unknown'
            
            sources.append(source_info)
        return sources

    @traceable(
        name="RAG_Generate_Answer",
        metadata={"component": "generator", "version": "2.0"}
    )
    def generate_answer(
        self, 
        query: str, 
        top_k: int = None,
        search_mode: str = None,
        alpha: float = None
    ) -> dict:
        """
        ë‹µë³€ ìƒì„± (Chain ê¸°ë°˜)
        
        Args:
            query: ì§ˆë¬¸
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            search_mode: ê²€ìƒ‰ ëª¨ë“œ ("embedding", "hybrid", "hybrid_rerank")
            alpha: ì„ë² ë”© ê°€ì¤‘ì¹˜ (0~1)
        
        Returns:
            dict: answer, sources, search_mode, usage
        """
        try:
            start_time = time.time()

            classification = self.router.classify(query)
            query_type = classification.get('type', 'document')

            # ë¹„ë¬¸ì„œ ì§ˆì˜ëŠ” ì¦‰ì‹œ ì‘ë‹µ
            if query_type != 'document':
                print(f"â­ï¸  ë¼ìš°í„°: ê²€ìƒ‰ ìƒëµ ({query_type})")
                answer = self._direct_responses.get(
                    query_type,
                    self._direct_responses['out_of_scope']
                )
                elapsed_time = time.time() - start_time
                self._last_retrieved_docs = []

                self.chat_history.append({"role": "user", "content": query})
                self.chat_history.append({"role": "assistant", "content": answer})

                return {
                    'answer': answer,
                    'sources': [],
                    'search_mode': 'none',
                    'elapsed_time': elapsed_time,
                    'usage': {
                        'total_tokens': 0,
                        'prompt_tokens': 0,
                        'completion_tokens': 0
                    },
                    'routing': classification
                }

            # íŒŒë¼ë¯¸í„° ì„¤ì •
            if top_k is not None:
                self.top_k = top_k
            if search_mode is not None:
                self.search_mode = search_mode
            if alpha is not None:
                self.alpha = alpha
            
            # Chain ì‹¤í–‰
            answer = self.chain.invoke(query)
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì•ˆì „ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
            if not self._last_retrieved_docs:
                answer = "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."
                print("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ì•ˆì „ ì‘ë‹µ ë°˜í™˜")
            
            elapsed_time = time.time() - start_time
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.chat_history.append({"role": "user", "content": query})
            self.chat_history.append({"role": "assistant", "content": answer})
            
            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (LangChainì—ì„œëŠ” ì§ì ‘ ì ‘ê·¼ ì–´ë ¤ì›€)
            estimated_tokens = len(query.split()) + len(answer.split()) * 2
            
            return {
                'answer': answer,
                'sources': self._format_sources(self._last_retrieved_docs),
                'search_mode': self.search_mode,
                'elapsed_time': elapsed_time,
                'usage': {
                    'total_tokens': estimated_tokens,
                    'prompt_tokens': 0,
                    'completion_tokens': 0
                },
                'routing': classification
            }
        
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}") from e

    def chat(self, query: str) -> str:
        """
        ê°„ë‹¨í•œ ëŒ€í™” ì¸í„°í˜ì´ìŠ¤
        
        Args:
            query: ì§ˆë¬¸
        
        Returns:
            str: ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
        """
        result = self.generate_answer(query)
        return result['answer']

    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.chat_history = []
        print("ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def get_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.chat_history.copy()

    def set_search_config(self, search_mode: str = None, top_k: int = None, alpha: float = None):
        """ê²€ìƒ‰ ì„¤ì • ë³€ê²½"""
        if search_mode is not None:
            self.search_mode = search_mode
        if top_k is not None:
            self.top_k = top_k
        if alpha is not None:
            self.alpha = alpha
        
        print(f"ğŸ”§ ê²€ìƒ‰ ì„¤ì • ë³€ê²½: mode={self.search_mode}, top_k={self.top_k}, alpha={self.alpha}")

    def print_result(self, result: dict, query: str = None):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        if query:
            print(f"ì§ˆë¬¸: {query}")
        print(f"ê²€ìƒ‰ ëª¨ë“œ: {result.get('search_mode', 'N/A')}")
        if 'elapsed_time' in result:
            print(f"ì†Œìš” ì‹œê°„: {result['elapsed_time']:.2f}ì´ˆ")
        print("="*60)
        print(f"\nğŸ’¬ ë‹µë³€:\n{result['answer']}")
        print(f"\nğŸ“š ì°¸ê³  ë¬¸ì„œ ({len(result['sources'])}ê°œ):")
        for i, source in enumerate(result['sources'], 1):
            score = source.get('score', 0)
            score_type = source.get('score_type', '')
            print(f"  [{i}] {source['filename']}")
            print(f"      ì ìˆ˜: {score:.3f} ({score_type})")
        print("="*60)


# ëŒ€í™”í˜• ì‹¤í–‰
def interactive_mode():
    """ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰"""
    print("=" * 60)
    print("ëŒ€í™”í˜• RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    print("=" * 60)
    
    config = RAGConfig()
    pipeline = RAGPipeline(config=config)
    
    print("\n" + "=" * 60)
    print("ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘")
    print("ëª…ë ¹ì–´: 'quit' (ì¢…ë£Œ), 'clear' (íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”), 'mode' (ê²€ìƒ‰ëª¨ë“œ ë³€ê²½)")
    print("=" * 60)
    
    while True:
        user_query = input("\nì§ˆë¬¸: ").strip()
        
        if not user_query:
            continue
        
        if user_query.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
            print("ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if user_query.lower() == 'clear':
            pipeline.clear_history()
            continue
        
        if user_query.lower() == 'mode':
            print("\nê²€ìƒ‰ ëª¨ë“œ ì„ íƒ:")
            print("1. embedding - ì„ë² ë”© ê²€ìƒ‰")
            print("2. hybrid - BM25 + ì„ë² ë”©")
            print("3. hybrid_rerank - Hybrid + Re-ranker (ê¶Œì¥)")
            choice = input("ì„ íƒ (1/2/3): ").strip()
            modes = {'1': 'embedding', '2': 'hybrid', '3': 'hybrid_rerank'}
            if choice in modes:
                pipeline.set_search_config(search_mode=modes[choice])
            continue
        
        try:
            result = pipeline.generate_answer(query=user_query)
            pipeline.print_result(result, user_query)
            
            # ì†ŒìŠ¤ ì¶œë ¥ ì—¬ë¶€
            show_source = input("\nì°¸ì¡° ë¬¸ì„œ ìƒì„¸ ë³´ê¸°? (y/n): ").strip().lower()
            if show_source == 'y':
                for i, source in enumerate(result['sources'], 1):
                    print(f"\n{'='*40}")
                    print(f"[ë¬¸ì„œ {i}] {source['filename']}")
                    print(f"ë°œì£¼ê¸°ê´€: {source['organization']}")
                    print(f"ë‚´ìš©:\n{source['content'][:500]}...")
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    interactive_mode()
