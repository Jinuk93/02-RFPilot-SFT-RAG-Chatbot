"""
RAG ë°ì´í„° ì „ì²˜ë¦¬ ì „ì²´ íŒŒì´í”„ë¼ì¸
í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ì •ì œ â†’ ì²­í‚¹ â†’ ì €ì¥

ëª¨ë“  ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ í†µí•©
"""

import os
import re
import zlib
import struct
import pandas as pd
from tqdm import tqdm
from pypdf import PdfReader
import olefile
from langchain_text_splitters import RecursiveCharacterTextSplitter

from src.utils.config import PreprocessConfig


# ============================================================
# í…ìŠ¤íŠ¸ ì¶”ì¶œ í´ë˜ìŠ¤
# ============================================================

class TextExtractor:
    """PDF ë° HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    
    @staticmethod
    def extract_pdf(filepath: str) -> str:
        """
        PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            filepath: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            reader = PdfReader(filepath)
            page_texts = [
                page.extract_text() 
                for page in reader.pages 
                if page.extract_text()
            ]
            return "\n\n".join(page_texts)
        except Exception as e:
            return f"[PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}]"
    
    @staticmethod
    def extract_hwp(filepath: str) -> str:
        """
        HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            filepath: HWP íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            f = olefile.OleFileIO(filepath)
            dirs = f.listdir()
            
            # HWP 5.0 íŒŒì¼ ê²€ì¦
            if ["FileHeader"] not in dirs or ["\x05HwpSummaryInformation"] not in dirs:
                return "[HWP ì¶”ì¶œ ì‹¤íŒ¨: ìœ íš¨í•œ HWP 5.0 íŒŒì¼ì´ ì•„ë‹˜]"
            
            # ì••ì¶• ì—¬ë¶€ í™•ì¸
            header = f.openstream("FileHeader")
            header_data = header.read()
            is_compressed = (header_data[36] & 1) == 1
            
            # ì„¹ì…˜ ë²ˆí˜¸ ì •ë ¬
            nums = [
                int(d[1][len("Section"):]) 
                for d in dirs 
                if d[0] == "BodyText"
            ]
            sections = [f"BodyText/Section{x}" for x in sorted(nums)]
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = ""
            for section in sections:
                bodytext = f.openstream(section)
                data = bodytext.read()
                
                # ì••ì¶• í•´ì œ
                if is_compressed:
                    unpacked_data = zlib.decompress(data, -15)
                else:
                    unpacked_data = data
                
                # ë ˆì½”ë“œ íŒŒì‹±
                i = 0
                size = len(unpacked_data)
                while i < size:
                    header = struct.unpack_from("<I", unpacked_data, i)[0]
                    rec_type = header & 0x3ff
                    rec_len = (header >> 20) & 0xfff
                    
                    # í…ìŠ¤íŠ¸ ë ˆì½”ë“œ (íƒ€ì… 67)
                    if rec_type == 67:
                        rec_data = unpacked_data[i + 4 : i + 4 + rec_len]
                        text += rec_data.decode('utf-16', errors='ignore')
                    
                    i += 4 + rec_len
            
            f.close()
            return text
            
        except Exception as e:
            return f"[HWP ì¶”ì¶œ ì‹¤íŒ¨: {e}]"
    
    @staticmethod
    def extract(filepath: str, file_format: str) -> str:
        """
        íŒŒì¼ í˜•ì‹ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            filepath: íŒŒì¼ ê²½ë¡œ
            file_format: íŒŒì¼ í˜•ì‹ ('pdf' ë˜ëŠ” 'hwp')
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        if not os.path.exists(filepath):
            return "[ì¶”ì¶œ ì‹¤íŒ¨: íŒŒì¼ ì—†ìŒ]"
        
        file_format = file_format.lower()
        
        if file_format == 'pdf':
            return TextExtractor.extract_pdf(filepath)
        elif file_format == 'hwp':
            return TextExtractor.extract_hwp(filepath)
        else:
            return f"[ì¶”ì¶œ ì‹¤íŒ¨: ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹ ({file_format})]"


# ============================================================
# í…ìŠ¤íŠ¸ ì •ì œ í´ë˜ìŠ¤
# ============================================================

class TextCleaner:
    """í…ìŠ¤íŠ¸ ì •ì œ ë° ê²€ì¦"""
    
    @staticmethod
    def clean(text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ì œ
        - íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ê³µë°±ë¬¸ìë§Œ ìœ ì§€)
        - NULL ë¬¸ì ì œê±°
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì •ì œëœ í…ìŠ¤íŠ¸
        """
        # í—ˆìš©: ì˜ë¬¸, ìˆ«ì, ê³µë°±, íƒ­, ì¤„ë°”ê¿ˆ, í•œê¸€
        cleaned = re.sub(
            r'[^\x20-\x7E\n\r\t\uAC00-\uD7AF]', 
            '', 
            str(text)
        )
        
        # NULL ë¬¸ì ì œê±°
        cleaned = cleaned.replace('\x00', '')
        
        return cleaned
    
    @staticmethod
    def validate(text: str, min_length: int = 100) -> bool:
        """
        í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì‚¬
        
        Args:
            text: ê²€ì¦í•  í…ìŠ¤íŠ¸
            min_length: ìµœì†Œ ê¸¸ì´
            
        Returns:
            ìœ íš¨ ì—¬ë¶€
        """
        if not text or text.strip() == "":
            return False
        
        if "[ì¶”ì¶œ ì‹¤íŒ¨" in text:
            return False
        
        if len(text) < min_length:
            return False
        
        return True
    
    @staticmethod
    def get_stats(text: str) -> dict:
        """
        í…ìŠ¤íŠ¸ í†µê³„ ì •ë³´
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        return {
            'length': len(text),
            'lines': text.count('\n') + 1,
            'words': len(text.split()),
            'is_valid': TextCleaner.validate(text)
        }


# ============================================================
# ë¬¸ì„œ ì²­í‚¹ í´ë˜ìŠ¤
# ============================================================

class DocumentChunker:
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
    
    def __init__(self, config: PreprocessConfig):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: ì „ì²˜ë¦¬ ì„¤ì • ê°ì²´
        """
        self.config = config
        
        # LangChain í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            separators=config.SEPARATORS,
            length_function=len,
        )
    
    def chunk_document(self, text: str, metadata: dict) -> list:
        """
        ë‹¨ì¼ ë¬¸ì„œ ì²­í‚¹
        
        Args:
            text: ë¬¸ì„œ í…ìŠ¤íŠ¸
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            chunks = self.splitter.split_text(text)
        except Exception as e:
            print(f"WARNING: ë¬¸ì„œ ë¶„í•  ì‹¤íŒ¨ - {e}")
            return []
        
        chunk_records = []
        filename = metadata.get('íŒŒì¼ëª…', 'unknown')
        
        for i, chunk_content in enumerate(chunks, 1):
            chunk_record = metadata.copy()
            chunk_record['chunk_id'] = f"{filename}_chunk_{i:04d}"
            chunk_record['chunk_content'] = chunk_content
            chunk_records.append(chunk_record)
        
        return chunk_records
    
    def chunk_dataframe(
        self, 
        df: pd.DataFrame, 
        text_column: str = 'text_content'
    ) -> pd.DataFrame:
        """
        DataFrame ì „ì²´ ì²­í‚¹
        
        Args:
            df: ì›ë³¸ DataFrame
            text_column: í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆëŠ” ì»¬ëŸ¼ëª…
            
        Returns:
            ì²­í¬ DataFrame
        """
        print(f"ì²­í‚¹ ì‹œì‘ (í¬ê¸°: {self.config.CHUNK_SIZE}, "
              f"ì˜¤ë²„ë©: {self.config.CHUNK_OVERLAP})...")
        
        all_chunks = []
        
        for index, row in tqdm(df.iterrows(), total=len(df), desc="ì²­í‚¹"):
            text = row[text_column]
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì œì™¸)
            metadata = row.to_dict()
            metadata.pop(text_column, None)
            metadata.pop('text_length', None)
            
            # ì²­í‚¹
            chunks = self.chunk_document(text, metadata)
            all_chunks.extend(chunks)
        
        df_chunks = pd.DataFrame(all_chunks)
        
        print(f"ì²­í‚¹ ì™„ë£Œ: ì›ë³¸ {len(df)}ê°œ â†’ ì²­í¬ {len(df_chunks)}ê°œ")
        
        return df_chunks


# ============================================================
# RAG ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# ============================================================

class RAGPreprocessPipeline:
    """RAG ë°ì´í„° ì „ì²˜ë¦¬ ì „ì²´ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: PreprocessConfig = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: ì „ì²˜ë¦¬ ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’)
        """
        self.config = config or PreprocessConfig()
        self.extractor = TextExtractor()
        self.cleaner = TextCleaner()
        self.chunker = DocumentChunker(self.config)
        
        # í†µê³„ ì •ë³´
        self.stats = {
            'total_files': 0,
            'success_files': 0,
            'failed_files': 0,
            'total_chunks': 0
        }
    
    def extract_from_files(self) -> pd.DataFrame:
        """
        1ë‹¨ê³„: íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Returns:
            í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œëœ DataFrame
        """
        print("\n" + "="*60)
        print("1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ")
        print("="*60)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(self.config.META_CSV_PATH)
        self.stats['total_files'] = len(df)
        print(f"íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")
        
        extracted_data = []
        
        for index, row in tqdm(df.iterrows(), total=len(df), desc="í…ìŠ¤íŠ¸ ì¶”ì¶œ"):
            filepath = os.path.join(self.config.BASE_FOLDER_PATH, row['íŒŒì¼ëª…'])
            file_format = row['íŒŒì¼í˜•ì‹']
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            raw_text = self.extractor.extract(filepath, file_format)
            
            # ì •ì œ
            cleaned_text = self.cleaner.clean(raw_text)
            
            # HWP íŠ¹ìˆ˜ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼)
            if file_format == 'hwp' and len(cleaned_text) < self.config.MIN_TEXT_LENGTH:
                if "[ì¶”ì¶œ ì‹¤íŒ¨" not in cleaned_text:
                    cleaned_text = "[ì¶”ì¶œ ì‹¤íŒ¨: HWP í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ]"
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if self.cleaner.validate(cleaned_text):
                self.stats['success_files'] += 1
            else:
                self.stats['failed_files'] += 1
            
            # ê²°ê³¼ ì €ì¥
            new_row = row.to_dict()
            new_row['full_text'] = cleaned_text
            
            # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
            if 'í…ìŠ¤íŠ¸' in new_row:
                del new_row['í…ìŠ¤íŠ¸']
            
            extracted_data.append(new_row)
        
        result_df = pd.DataFrame(extracted_data)
        
        print(f"\ní…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ:")
        print(f"  - ì„±ê³µ: {self.stats['success_files']}ê°œ")
        print(f"  - ì‹¤íŒ¨: {self.stats['failed_files']}ê°œ")
        
        return result_df
    
    def clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        2ë‹¨ê³„: DataFrame ì •ì œ
        
        Args:
            df: ì›ë³¸ DataFrame
            
        Returns:
            ì •ì œëœ DataFrame
        """
        print("\n" + "="*60)
        print("2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ")
        print("="*60)
        
        # ì»¬ëŸ¼ëª… ë³€ê²½
        df['text_content'] = df['full_text']
        df = df.drop(columns=['full_text'])
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df['text_content'] = df['text_content'].fillna('')
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        df['text_length'] = df['text_content'].apply(len)
        
        print(f"í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ")
        print(f"  - í‰ê·  ê¸¸ì´: {df['text_length'].mean():.0f} ë¬¸ì")
        print(f"  - ìµœì†Œ ê¸¸ì´: {df['text_length'].min()} ë¬¸ì")
        print(f"  - ìµœëŒ€ ê¸¸ì´: {df['text_length'].max()} ë¬¸ì")
        
        return df
    
    def create_chunks(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        3ë‹¨ê³„: ì²­í‚¹
        
        Args:
            df: ì •ì œëœ DataFrame
            
        Returns:
            ì²­í¬ DataFrame
        """
        print("\n" + "="*60)
        print("3ë‹¨ê³„: ì²­í‚¹")
        print("="*60)
        
        # [ì¶”ê°€] í•„í„°ë§ ì „ ìƒíƒœ í™•ì¸
        original_count = len(df)
        print(f"ğŸ” í•„í„°ë§ ì „ ë¬¸ì„œ ìˆ˜: {original_count}")
        
        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
        if len(df) > 0:
            sample = df['text_content'].iloc[0]
            print(f"ğŸ” ì²« ë²ˆì§¸ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°:")
            print(f"   ì‹œì‘ ë¶€ë¶„: {sample[:100]}...")
            print(f"   ì „ì²´ ê¸¸ì´: {len(sample)}ì")
            
            # ì¶”ì¶œ ì‹¤íŒ¨ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
            has_failure = any([
                '[ì¶”ì¶œ ì‹¤íŒ¨' in sample,
                '[PDF ì¶”ì¶œ ì‹¤íŒ¨' in sample,
                '[HWP ì¶”ì¶œ ì‹¤íŒ¨' in sample
            ])
            print(f"   ì¶”ì¶œ ì‹¤íŒ¨ í¬í•¨?: {has_failure}")
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ë¬¸ì„œ í•„í„°ë§ (raw string ì‚¬ìš©)
        df = df[~df['text_content'].str.contains(r'\[ì¶”ì¶œ ì‹¤íŒ¨', na=False)]
        df = df[~df['text_content'].str.contains(r'\[PDF ì¶”ì¶œ ì‹¤íŒ¨', na=False)]
        df = df[~df['text_content'].str.contains(r'\[HWP ì¶”ì¶œ ì‹¤íŒ¨', na=False)]
        
        filtered_count = original_count - len(df)
        
        print(f"\nğŸ“Š í•„í„°ë§ ê²°ê³¼:")
        print(f"   ì œì™¸ëœ ë¬¸ì„œ: {filtered_count}ê°œ")
        print(f"   ë‚¨ì€ ë¬¸ì„œ: {len(df)}ê°œ")
        
        if len(df) == 0:
            print("\nâŒ ê²½ê³ : ëª¨ë“  ë¬¸ì„œê°€ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("   â†’ ì¶”ì¶œì´ ëª¨ë‘ ì‹¤íŒ¨í–ˆê±°ë‚˜ í•„í„°ë§ ì¡°ê±´ì´ ë„ˆë¬´ ì—„ê²©í•©ë‹ˆë‹¤.")
            return pd.DataFrame()
        
        if filtered_count > 0:
            print(f"âš ï¸  ì¶”ì¶œ ì‹¤íŒ¨ ë¬¸ì„œ ì œì™¸: {filtered_count}ê°œ")
            print(f"âœ… ìœ íš¨í•œ ë¬¸ì„œ: {len(df)}ê°œ")
        
        # ì²­í‚¹ ì‹œì‘
        df_chunks = self.chunker.chunk_dataframe(df)
        self.stats['total_chunks'] = len(df_chunks)
        
        return df_chunks
    
    def save_chunks(self, df_chunks: pd.DataFrame):
        """
        4ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        
        Args:
            df_chunks: ì²­í¬ DataFrame
        """
        print("\n" + "="*60)
        print("4ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
        print("="*60)
        
        df_chunks.to_csv(
            self.config.OUTPUT_CHUNKS_PATH, 
            index=False, 
            encoding='utf-8-sig'
        )
        
        print(f"ìµœì¢… ì²­í¬ ì €ì¥ ì™„ë£Œ: {self.config.OUTPUT_CHUNKS_PATH}")
        print(f"ì´ ì²­í¬ ìˆ˜: {len(df_chunks)}")
    
    def run(self) -> pd.DataFrame:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Returns:
            ìµœì¢… ì²­í¬ DataFrame
        """
        print("="*60)
        print("RAG ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("="*60)
        
        # ì„¤ì • ê²€ì¦
        self.config.validate()
        print(self.config)
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        df_extracted = self.extract_from_files()
        
        # 2. í…ìŠ¤íŠ¸ ì •ì œ
        df_cleaned = self.clean_dataframe(df_extracted)
        
        # 3. ì²­í‚¹
        df_chunks = self.create_chunks(df_cleaned)
        
        # 4. ì €ì¥
        self.save_chunks(df_chunks)
        
        # ìµœì¢… í†µê³„
        self._print_final_stats()
        
        print("\n" + "="*60)
        print("âœ… RAG ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        print("="*60)
        
        return df_chunks
    
    def _print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ìµœì¢… í†µê³„")
        print("="*60)
        print(f"ì´ íŒŒì¼ ìˆ˜: {self.stats['total_files']}")
        
        if self.stats['total_files'] > 0:
            success_rate = self.stats['success_files'] / self.stats['total_files'] * 100
            fail_rate = self.stats['failed_files'] / self.stats['total_files'] * 100
            
            print(f"  - ì¶”ì¶œ ì„±ê³µ: {self.stats['success_files']} ({success_rate:.1f}%)")
            print(f"  - ì¶”ì¶œ ì‹¤íŒ¨: {self.stats['failed_files']} ({fail_rate:.1f}%)")
        
        print(f"ì´ ì²­í¬ ìˆ˜: {self.stats['total_chunks']}")
        
        if self.stats['success_files'] > 0:
            avg_chunks = self.stats['total_chunks'] / self.stats['success_files']
            print(f"íŒŒì¼ë‹¹ í‰ê·  ì²­í¬: {avg_chunks:.1f}ê°œ")