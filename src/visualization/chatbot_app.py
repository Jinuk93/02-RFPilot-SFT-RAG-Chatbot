"""
ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œ RAG ì±—ë´‡

ê¸°ëŠ¥:
- ëª¨ë¸ ì„ íƒ (API/ë¡œì»¬ GGUF)
- Query Router (ê²€ìƒ‰ vs ì§ì ‘ ë‹µë³€)
- RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (Hybrid Search + Re-ranker)
- ì¡°ê±´ë¶€ ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ
"""

import streamlit as st
import sys
from pathlib import Path
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

from src.utils.config import RAGConfig
from src.utils.conversation_manager import ConversationManager


# ===== í˜ì´ì§€ ì„¤ì • =====
st.set_page_config(
    page_title="ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)


# ===== ìŠ¤íƒ€ì¼ =====
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        display: flex;
        flex-direction: column;
    }
    .user-message {
        background-color: #e3f2fd;
        border-left: 5px solid #2196f3;
    }
    .assistant-message {
        background-color: #f5f5f5;
        border-left: 5px solid #4caf50;
    }
    .message-header {
        font-weight: bold;
        margin-bottom: 0.5rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
    }
    .message-content {
        line-height: 1.6;
    }
    .source-document {
        background-color: #fff9c4;
        padding: 1rem;
        border-radius: 0.3rem;
        margin: 0.5rem 0;
        border-left: 3px solid #fbc02d;
    }
    .source-header {
        font-weight: bold;
        color: #f57f17;
        margin-bottom: 0.5rem;
    }
    .metadata {
        font-size: 0.85rem;
        color: #666;
        margin-top: 0.5rem;
    }
    .token-usage {
        background-color: #e8f5e9;
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        font-size: 0.9rem;
        margin-top: 0.5rem;
    }
    .search-mode-info {
        background-color: #e3f2fd;
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        font-size: 0.9rem;
        margin-top: 0.5rem;
    }
    .routing-info {
        background-color: #fff3e0;
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        font-size: 0.9rem;
        margin-top: 0.5rem;
        border-left: 3px solid #ff9800;
    }
    .model-info {
        background-color: #f3e5f5;
        padding: 0.8rem 1rem;
        border-radius: 0.3rem;
        font-size: 0.9rem;
        margin: 0.5rem 0;
        border-left: 3px solid #9c27b0;
    }
</style>
""", unsafe_allow_html=True)


# ===== ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” =====
if 'conv_manager' not in st.session_state:
    st.session_state.conv_manager = ConversationManager()

if 'rag_pipeline' not in st.session_state:
    st.session_state.rag_pipeline = None

if 'model_type' not in st.session_state:
    st.session_state.model_type = None

if 'show_routing_info' not in st.session_state:
    st.session_state.show_routing_info = False


# ===== RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” =====
@st.cache_resource
def initialize_rag(model_type):
    """
    RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    
    Args:
        model_type: "API ëª¨ë¸ (GPT)" ë˜ëŠ” "ë¡œì»¬ ëª¨ë¸ (GGUF)"
    
    Returns:
        (rag_pipeline, error_message, model_name)
    """
    try:
        config = RAGConfig()
        
        if model_type == "API ëª¨ë¸ (GPT)":
            # API ëª¨ë¸ ì‚¬ìš©
            from src.generator.generator import RAGPipeline
            rag = RAGPipeline(config=config)
            return rag, None, "OpenAI GPT"
            
        elif model_type == "ë¡œì»¬ ëª¨ë¸ (GGUF)":
            # GGUF ëª¨ë¸ ì‚¬ìš©
            from src.generator.generator_gguf import GGUFRAGPipeline
            
            # T4 GPU ìµœì  ì„¤ì •
            rag = GGUFRAGPipeline(
                config=config,
                n_gpu_layers=35,  # T4ì—ì„œ ì „ì²´ ë ˆì´ì–´ GPU ì‚¬ìš©
                n_ctx=4096,       # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
                n_threads=4,      # CPU ìŠ¤ë ˆë“œ (GPU ì‚¬ìš© ì‹œ ë‚®ê²Œ)
                max_new_tokens=512,  # ìµœëŒ€ ìƒì„± í† í°
                temperature=0.7,
                top_p=0.9
            )
            return rag, None, "Llama-3-Ko-8B (GGUF)"
        
        else:
            return None, f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {model_type}", None
            
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        return None, f"{str(e)}\n\n{error_detail}", None


# ===== ë‹µë³€ ìƒì„± =====
def generate_answer(query: str, top_k: int = 10, search_mode: str = "hybrid_rerank", alpha: float = 0.5):
    """ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
    try:
        result = st.session_state.rag_pipeline.generate_answer(
            query=query,
            top_k=top_k,
            search_mode=search_mode,
            alpha=alpha
        )
        return result
        
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        return {
            'answer': f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\n{error_detail}",
            'sources': [],
            'used_retrieval': False,
            'search_mode': search_mode,
            'routing_info': None,
            'usage': {'total_tokens': 0, 'prompt_tokens': 0, 'completion_tokens': 0}
        }


# ===== ë©”ì‹œì§€ í‘œì‹œ =====
def display_message(
    role: str, 
    content: str, 
    sources: list = None, 
    usage: dict = None, 
    search_mode: str = None,
    used_retrieval: bool = None,
    routing_info: dict = None
):
    """
    ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œ
    
    Args:
        role: 'user' ë˜ëŠ” 'assistant'
        content: ë©”ì‹œì§€ ë‚´ìš©
        sources: ì°¸ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (assistantë§Œ)
        usage: í† í° ì‚¬ìš©ëŸ‰ (assistantë§Œ)
        search_mode: ê²€ìƒ‰ ëª¨ë“œ (assistantë§Œ)
        used_retrieval: ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ (assistantë§Œ)
        routing_info: ë¼ìš°íŒ… ì •ë³´ (assistantë§Œ)
    """
    if role == 'user':
        st.markdown(f"""
        <div class="chat-message user-message">
            <div class="message-header">
                ğŸ‘¤ ì‚¬ìš©ì
            </div>
            <div class="message-content">
                {content}
            </div>
        </div>
        """, unsafe_allow_html=True)
        
    else:  # assistant
        # ë‹µë³€
        st.markdown(f"""
        <div class="chat-message assistant-message">
            <div class="message-header">
                ğŸ¤– ì±—ë´‡
            </div>
            <div class="message-content">
                {content}
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # ===== ë¼ìš°íŒ… ì •ë³´ (ê°œë°œ ëª¨ë“œ) =====
        if st.session_state.show_routing_info and routing_info:
            route_icon = "ğŸ”" if routing_info.get('route') == 'rag' else "ğŸ’¬"
            st.markdown(f"""
            <div class="routing-info">
                {route_icon} ë¼ìš°íŒ…: {routing_info.get('route', 'N/A').upper()} 
                (ì‹ ë¢°ë„: {routing_info.get('confidence', 0):.2f}) - 
                {routing_info.get('reason', 'N/A')}
            </div>
            """, unsafe_allow_html=True)
        
        # ===== ê²€ìƒ‰ ëª¨ë“œ ì •ë³´ (ê²€ìƒ‰ ì‚¬ìš© ì‹œë§Œ) =====
        if used_retrieval and search_mode:
            mode_display = {
                'hybrid_rerank': 'ğŸ”„ Hybrid + Re-ranker',
                'hybrid': 'ğŸ”€ Hybrid Search',
                'embedding_rerank': 'ğŸ“Š ì„ë² ë”© + Re-ranker',
                'embedding': 'ğŸ“Š ì„ë² ë”© ê²€ìƒ‰',
                'direct': 'ğŸ’¬ Direct (ê²€ìƒ‰ ì—†ìŒ)'
            }
            st.markdown(f"""
            <div class="search-mode-info">
                ê²€ìƒ‰ ëª¨ë“œ: {mode_display.get(search_mode, search_mode)}
            </div>
            """, unsafe_allow_html=True)
        
        # ===== ì°¸ê³  ë¬¸ì„œ (ê²€ìƒ‰ ì‚¬ìš© ì‹œë§Œ) =====
        if used_retrieval and sources and len(sources) > 0:
            st.markdown("### ğŸ“š ì°¸ê³  ë¬¸ì„œ")
            
            for i, source in enumerate(sources, 1):
                metadata = source.get('metadata', {})
                
                # ê´€ë ¨ë„ ì ìˆ˜
                score = source.get('score', 0)
                score_type = source.get('score_type', '')
                
                # ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                content_preview = source.get('content', '')[:200] + "..."
                
                st.markdown(f"""
                <div class="source-document">
                    <div class="source-header">
                        ğŸ“„ ë¬¸ì„œ {i} (ì ìˆ˜: {score:.3f} / {score_type})
                    </div>
                    <div>
                        {content_preview}
                    </div>
                    <div class="metadata">
                        ğŸ“ íŒŒì¼: {metadata.get('íŒŒì¼ëª…', 'N/A')}<br>
                        ğŸ¢ ë°œì£¼ê¸°ê´€: {metadata.get('ë°œì£¼ ê¸°ê´€', 'N/A')}<br>
                        ğŸ“‹ ì‚¬ì—…ëª…: {metadata.get('ì‚¬ì—…ëª…', 'N/A')}
                    </div>
                </div>
                """, unsafe_allow_html=True)
        elif not used_retrieval:
            # ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²½ìš° ì•ˆë‚´
            st.info("ğŸ’¬ ì´ ë‹µë³€ì€ ë¬¸ì„œ ê²€ìƒ‰ ì—†ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ===== í† í° ì‚¬ìš©ëŸ‰ =====
        if usage:
            st.markdown(f"""
            <div class="token-usage">
                ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {usage.get('total_tokens', 0)} 
                (í”„ë¡¬í”„íŠ¸: {usage.get('prompt_tokens', 0)}, 
                 ì™„ì„±: {usage.get('completion_tokens', 0)})
            </div>
            """, unsafe_allow_html=True)


# ===== ë©”ì¸ ì•± =====
def main():
    # í—¤ë”
    st.markdown('<div class="main-header">ğŸ¤– ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œ ì±—ë´‡</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">Query Router + RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ</div>', unsafe_allow_html=True)
    
    # ===== ì‚¬ì´ë“œë°” =====
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ëª¨ë¸ ì„¤ì •
        st.markdown("### ğŸ¤– ëª¨ë¸ ì„¤ì •")
        
        model_type = st.selectbox(
            "ìƒì„± ëª¨ë¸ ì„ íƒ",
            options=[
                "API ëª¨ë¸ (GPT)",
                "ë¡œì»¬ ëª¨ë¸ (GGUF)"
            ],
            index=0,
            help="OpenAI API ë˜ëŠ” ë¡œì»¬ GGUF ëª¨ë¸ ì„ íƒ"
        )
        
        # ëª¨ë¸ë³„ ì •ë³´ í‘œì‹œ
        if model_type == "API ëª¨ë¸ (GPT)":
            st.markdown("""
            <div class="model-info">
                ğŸŒ <b>OpenAI GPT ëª¨ë¸</b><br>
                â€¢ ë¹ ë¥´ê³  ì•ˆì •ì <br>
                â€¢ API í‚¤ í•„ìš”<br>
                â€¢ ë¹„ìš© ë°œìƒ (í† í°ë‹¹)
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown("""
            <div class="model-info">
                ğŸ–¥ï¸ <b>Llama-3-Ko-8B (GGUF)</b><br>
                â€¢ T4 GPU ê°€ì†<br>
                â€¢ ë¡œì»¬ ì‹¤í–‰ (ë¬´ë£Œ)<br>
                â€¢ ì´ˆê¸° ë¡œë”© ì‹œê°„ ì†Œìš”<br>
                â€¢ 35ê°œ ë ˆì´ì–´ GPU ì‚¬ìš©
            </div>
            """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # ê²€ìƒ‰ ì„¤ì •
        st.markdown("### ğŸ” ê²€ìƒ‰ ì„¤ì •")
        
        search_mode = st.selectbox(
            "ê²€ìƒ‰ ëª¨ë“œ",
            options=["hybrid", "embedding"],
            index=0,
            format_func=lambda x: {
                "hybrid": "ğŸ”€ Hybrid Search (BM25 + ì„ë² ë”©)",
                "embedding": "ğŸ“Š ì„ë² ë”© ê²€ìƒ‰"
            }[x],
            help="Hybrid: í‚¤ì›Œë“œ + ì˜ë¯¸ ê²€ìƒ‰ ë³‘í–‰ (ê¶Œì¥)"
        )
        
        # Reranker í† ê¸€
        use_reranker = st.toggle(
            "ğŸ”„ Re-ranker ì‚¬ìš©",
            value=True,
            help="ê²€ìƒ‰ ê²°ê³¼ë¥¼ CrossEncoderë¡œ ì¬ì •ë ¬í•˜ì—¬ ì •í™•ë„ í–¥ìƒ (ê¶Œì¥)"
        )
        
        # ì‹¤ì œ ê²€ìƒ‰ ëª¨ë“œ ê²°ì •
        if use_reranker:
            if search_mode == "hybrid":
                actual_search_mode = "hybrid_rerank"
            else:  # embedding
                actual_search_mode = "embedding_rerank"
        else:
            actual_search_mode = search_mode
        
        top_k = st.slider(
            "ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (Top-K)",
            min_value=1,
            max_value=20,
            value=10,
            help="ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜"
        )
        
        alpha = st.slider(
            "ì„ë² ë”© ê°€ì¤‘ì¹˜ (alpha)",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.1,
            help="0: BM25ë§Œ, 1: ì„ë² ë”©ë§Œ, 0.5: ë™ì¼ ê°€ì¤‘ì¹˜ (Hybrid ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)",
            disabled=(search_mode == "embedding")
        )
        
        st.markdown("---")
        
        # ê°œë°œì ì˜µì…˜
        st.markdown("### ğŸ› ï¸ ê°œë°œì ì˜µì…˜")
        
        show_routing = st.toggle(
            "ğŸ” ë¼ìš°íŒ… ì •ë³´ í‘œì‹œ",
            value=False,
            help="Routerì˜ íŒë‹¨ ê³¼ì •ì„ í‘œì‹œ (ë””ë²„ê¹…ìš©)"
        )
        st.session_state.show_routing_info = show_routing
        
        st.markdown("---")
        
        # ëŒ€í™” ê´€ë¦¬
        st.markdown("### ğŸ’¬ ëŒ€í™” ê´€ë¦¬")
        
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.conv_manager.clear()
            st.rerun()
        
        if st.button("ğŸ’¾ ëŒ€í™” ë‹¤ìš´ë¡œë“œ", use_container_width=True):
            if len(st.session_state.conv_manager) > 0:
                json_str = st.session_state.conv_manager.export_to_json()
                
                st.download_button(
                    label="ğŸ“¥ JSON ë‹¤ìš´ë¡œë“œ",
                    data=json_str,
                    file_name=f"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json",
                    use_container_width=True
                )
        
        st.markdown("---")
        
        # í†µê³„
        st.markdown("### ğŸ“Š í†µê³„")
        stats = st.session_state.conv_manager.get_statistics()

        st.metric("ì´ ëŒ€í™” ìˆ˜", stats.get('total', 0))
        
        # í˜„ì¬ ì„¤ì • í‘œì‹œ
        st.markdown("---")
        st.markdown("### ğŸ“‹ í˜„ì¬ ì„¤ì •")
        st.text(f"ëª¨ë¸: {model_type}")
        st.text(f"ê²€ìƒ‰ ëª¨ë“œ: {search_mode}")
        st.text(f"Re-ranker: {'âœ… ON' if use_reranker else 'âŒ OFF'}")
        st.text(f"ì‹¤ì œ ëª¨ë“œ: {actual_search_mode}")
        st.text(f"Top-K: {top_k}")
        if search_mode == "hybrid":
            st.text(f"Alpha: {alpha}")
        st.text(f"Router Info: {'âœ… ON' if show_routing else 'âŒ OFF'}")
    
    # ===== RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” =====
    # ëª¨ë¸ íƒ€ì…ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ íŒŒì´í”„ë¼ì¸ì´ ì—†ìœ¼ë©´ ì¬ì´ˆê¸°í™”
    if (st.session_state.rag_pipeline is None or 
        st.session_state.model_type != model_type):
        
        with st.spinner(f"ğŸ”„ {model_type} ì´ˆê¸°í™” ì¤‘... (GGUF ëª¨ë¸ì€ 1~2ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            rag, error, rag_type = initialize_rag(model_type)
            
            if error:
                st.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                
                with st.expander("ğŸ” ì—ëŸ¬ ìƒì„¸ ì •ë³´"):
                    st.code(error)
                
                st.info("""
                ### ğŸ’¡ í•´ê²° ë°©ë²•
                
                **GGUF ëª¨ë¸ ì‹¤íŒ¨ ì‹œ:**
                1. llama-cpp-python ì„¤ì¹˜ í™•ì¸:
```bash
pip install llama-cpp-python
```
                
                2. GGUF ëª¨ë¸ íŒŒì¼ í™•ì¸:
                   - config.yamlì˜ GGUF_MODEL_PATH ë˜ëŠ”
                   - MODEL_HUB_REPO ì„¤ì • í™•ì¸
                
                3. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ:
                   - n_gpu_layers ê°’ ê°ì†Œ (35 â†’ 20)
                
                **API ëª¨ë¸ ì‹¤íŒ¨ ì‹œ:**
                1. ChromaDBê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸:
```bash
python main.py --step embed
```
                
                2. OpenAI API í‚¤ í™•ì¸:
```bash
# .env íŒŒì¼
OPENAI_API_KEY=your-key-here
```
                
                3. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:
```bash
pip install rank-bm25 sentence-transformers
```
                """)
                return
            
            st.session_state.rag_pipeline = rag
            st.session_state.model_type = model_type
            st.success(f"âœ… {rag_type} ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
    
    # ===== ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ =====
    st.markdown("---")
    
    if len(st.session_state.conv_manager) == 0:
        st.info("""
        ### ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!
        
        ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.
        
        **ì˜ˆì‹œ ì§ˆë¬¸:**
        - "ì•ˆë…•í•˜ì„¸ìš”" (ê²€ìƒ‰ ì•ˆ í•¨)
        - "ë°ì´í„° í‘œì¤€í™” ìš”êµ¬ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?" (ê²€ìƒ‰ ìˆ˜í–‰)
        - "ë³´ì•ˆ ê´€ë ¨ ìš”êµ¬ì‚¬í•­ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”" (ê²€ìƒ‰ ìˆ˜í–‰)
        - "ê³ ë§ˆì›Œìš”" (ê²€ìƒ‰ ì•ˆ í•¨)
        """)
    
    # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
    for msg in st.session_state.conv_manager.get_ui_history():
        display_message(
            role=msg['role'],
            content=msg['content'],
            sources=msg.get('sources'),
            usage=msg.get('usage'),
            search_mode=msg.get('search_mode'),
            used_retrieval=msg.get('used_retrieval'),
            routing_info=msg.get('routing_info')
        )
    
    # ===== ì§ˆë¬¸ ì…ë ¥ =====
    st.markdown("---")
    
    with st.form(key='question_form', clear_on_submit=True):
        user_input = st.text_area(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            height=100,
            placeholder="ì˜ˆ: ë°ì´í„° í‘œì¤€í™” ìš”êµ¬ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        )
        
        col1, col2, col3 = st.columns([1, 1, 4])
        
        with col1:
            submit_button = st.form_submit_button("ğŸ“¤ ì „ì†¡", use_container_width=True)
    
    # ===== ì§ˆë¬¸ ì²˜ë¦¬ =====
    if submit_button and user_input:

        # ë‹µë³€ ìƒì„±
        with st.spinner("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘..."):
            result = generate_answer(
                query=user_input,
                top_k=top_k,
                search_mode=actual_search_mode,
                alpha=alpha
            )
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.conv_manager.add_message(
            user_msg=user_input,
            ai_msg=result['answer'],
            query_type=result.get('query_type', 'unknown'),
            sources=result.get('sources', []),
            usage=result.get('usage', {}),
            search_mode=result.get('search_mode'),
            used_retrieval=result.get('used_retrieval', False),
            routing_info=result.get('routing_info')
        )
        
        # í™”ë©´ ìƒˆë¡œê³ ì¹¨
        st.rerun()


if __name__ == "__main__":
    main()