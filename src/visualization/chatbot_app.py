"""
ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œ RAG ì±—ë´‡

ê¸°ëŠ¥:
- ì‚¬ìš©ì API í‚¤ ì…ë ¥ ë° ê²€ì¦
- ì‚¬ìš© ê°€ëŠ¥í•œ GPT ëª¨ë¸ ìë™ ì¡°íšŒ ë° ì„ íƒ
- ëª¨ë¸ ì„ íƒ (API/ë¡œì»¬ GGUF)
- Query Router (ê²€ìƒ‰ vs ì§ì ‘ ë‹µë³€)
- RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (Hybrid Search + Re-ranker)
- ì¡°ê±´ë¶€ ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ
"""

import streamlit as st
import sys
import os
from pathlib import Path
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

from src.utils.config import RAGConfig
from src.utils.conversation_manager import ConversationManager


# ===== í˜ì´ì§€ ì„¤ì • =====
st.set_page_config(
    page_title="ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)


# ===== ìŠ¤íƒ€ì¼ =====
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        display: flex;
        flex-direction: column;
    }
    .user-message {
        background-color: #e3f2fd;
        border-left: 5px solid #2196f3;
    }
    .assistant-message {
        background-color: #f5f5f5;
        border-left: 5px solid #4caf50;
    }
    .message-header {
        font-weight: bold;
        margin-bottom: 0.5rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
    }
    .message-content {
        line-height: 1.6;
    }
    .source-document {
        background-color: #fff9c4;
        padding: 1rem;
        border-radius: 0.3rem;
        margin: 0.5rem 0;
        border-left: 3px solid #fbc02d;
    }
    .source-header {
        font-weight: bold;
        color: #f57f17;
        margin-bottom: 0.5rem;
    }
    .metadata {
        font-size: 0.85rem;
        color: #666;
        margin-top: 0.5rem;
    }
    .token-usage {
        background-color: #e8f5e9;
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        font-size: 0.9rem;
        margin-top: 0.5rem;
    }
    .search-mode-info {
        background-color: #e3f2fd;
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        font-size: 0.9rem;
        margin-top: 0.5rem;
    }
    .routing-info {
        background-color: #fff3e0;
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        font-size: 0.9rem;
        margin-top: 0.5rem;
        border-left: 3px solid #ff9800;
    }
    .model-info {
        background-color: #f3e5f5;
        padding: 0.8rem 1rem;
        border-radius: 0.3rem;
        font-size: 0.9rem;
        margin: 0.5rem 0;
        border-left: 3px solid #9c27b0;
    }
</style>
""", unsafe_allow_html=True)


# ===== ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” =====
if 'conv_manager' not in st.session_state:
    st.session_state.conv_manager = ConversationManager()

if 'rag_pipeline' not in st.session_state:
    st.session_state.rag_pipeline = None

if 'model_type' not in st.session_state:
    st.session_state.model_type = None

if 'show_routing_info' not in st.session_state:
    st.session_state.show_routing_info = False

if 'user_api_key' not in st.session_state:
    st.session_state.user_api_key = None

if 'api_key_validated' not in st.session_state:
    st.session_state.api_key_validated = False

if 'available_models' not in st.session_state:
    st.session_state.available_models = []

if 'selected_gpt_model' not in st.session_state:
    st.session_state.selected_gpt_model = "gpt-4o-mini"

if 'custom_db_path' not in st.session_state:
    st.session_state.custom_db_path = None

if 'db_uploaded' not in st.session_state:
    st.session_state.db_uploaded = False


# ===== API í‚¤ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ í•¨ìˆ˜ =====
def get_available_models(api_key: str) -> tuple:
    """
    API í‚¤ë¡œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  GPT/o ì‹œë¦¬ì¦ˆ ëª¨ë¸ ì¡°íšŒ
    
    Args:
        api_key: OpenAI API í‚¤
    
    Returns:
        (success, model_list, error_message)
    """
    try:
        from openai import OpenAI
        
        client = OpenAI(api_key=api_key)
        
        # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
        models_response = client.models.list()
        
        # Chat Completion ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ í•„í„°ë§
        available_models = []
        
        for model in models_response.data:
            model_id = model.id
            
            # GPT ì‹œë¦¬ì¦ˆ, o1, o3 ì‹œë¦¬ì¦ˆë§Œ ì„ íƒ
            if (model_id.startswith('gpt-') or 
                model_id.startswith('o1-') or 
                model_id.startswith('o3-')):
                available_models.append(model_id)
        
        if not available_models:
            return False, [], "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ìš°ì„ ìˆœìœ„ ì •ë ¬ (ìµœì‹ /ê³ ê¸‰ ëª¨ë¸ ìš°ì„ )
        priority_map = {
            'o3': 1,
            'o1': 2,
            'gpt-5': 3,
            'gpt-4o': 4,
            'gpt-4o-mini': 5,
            'gpt-4-turbo': 6,
            'gpt-4': 7,
            'gpt-3.5-turbo': 8,
            'gpt-3.5': 9
        }
        
        def get_priority(model_name):
            for prefix, priority in priority_map.items():
                if model_name.startswith(prefix):
                    return priority
            return 99
        
        available_models.sort(key=get_priority)
        
        # ì¤‘ë³µ ì œê±° (ë‚ ì§œ ë²„ì „ ì¤‘ ê°€ì¥ ìµœê·¼ ê²ƒë§Œ)
        unique_models = []
        seen_bases = {}
        
        for model in available_models:
            # ê¸°ë³¸ ëª¨ë¸ëª… ì¶”ì¶œ (ë‚ ì§œ/ë²„ì „ ì œê±°)
            base = model
            for suffix in ['-preview', '-latest']:
                base = base.replace(suffix, '')
            
            # ë‚ ì§œ íŒ¨í„´ ì œê±° (ì˜ˆ: -20241120, -2024-11-20)
            import re
            base = re.sub(r'-\d{8}$', '', base)
            base = re.sub(r'-\d{4}-\d{2}-\d{2}$', '', base)
            
            # ê°™ì€ baseê°€ ì´ë¯¸ ìˆìœ¼ë©´ ë” ê¸´ ì´ë¦„ ì„ íƒ (ë³´í†µ ìµœì‹ )
            if base not in seen_bases or len(model) > len(seen_bases[base]):
                seen_bases[base] = model
        
        unique_models = list(seen_bases.values())
        unique_models.sort(key=get_priority)
        
        return True, unique_models, ""
        
    except Exception as e:
        error_msg = str(e)
        
        if "Incorrect API key" in error_msg:
            return False, [], "âŒ ì˜ëª»ëœ API í‚¤ì…ë‹ˆë‹¤."
        elif "insufficient_quota" in error_msg:
            return False, [], "âš ï¸ API í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
        else:
            return False, [], f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {error_msg}"


# ===== API í‚¤ ê²€ì¦ í•¨ìˆ˜ =====
def validate_api_key(api_key: str) -> tuple:
    """
    OpenAI API í‚¤ ìœ íš¨ì„± ê²€ì¦ ë° ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ
    
    Args:
        api_key: ê²€ì¦í•  API í‚¤
    
    Returns:
        (is_valid, message, available_models)
    """
    try:
        # ëª¨ë¸ ëª©ë¡ ì¡°íšŒë¡œ ê²€ì¦ (chat completionë³´ë‹¤ ê¶Œí•œ ìš”êµ¬ì‚¬í•­ ë‚®ìŒ)
        success, models, error = get_available_models(api_key)
        
        if not success:
            return False, error, []
        
        if len(models) == 0:
            return False, "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.", []
        
        return True, f"âœ… API í‚¤ê°€ ìœ íš¨í•©ë‹ˆë‹¤! ({len(models)}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)", models
        
    except Exception as e:
        error_msg = str(e)
        
        if "Incorrect API key" in error_msg or "invalid_api_key" in error_msg:
            return False, "âŒ ì˜ëª»ëœ API í‚¤ì…ë‹ˆë‹¤. ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.", []
        elif "insufficient_quota" in error_msg:
            return False, "âš ï¸ API í‚¤ëŠ” ìœ íš¨í•˜ì§€ë§Œ í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.", []
        elif "403" in error_msg or "Forbidden" in error_msg:
            return False, "âŒ API í‚¤ ê¶Œí•œì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í‚¤ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.", []
        else:
            return False, f"âŒ API í‚¤ ê²€ì¦ ì‹¤íŒ¨: {error_msg}", []
        
    except Exception as e:
        error_msg = str(e)
        
        if "Incorrect API key" in error_msg or "invalid_api_key" in error_msg:
            return False, "âŒ ì˜ëª»ëœ API í‚¤ì…ë‹ˆë‹¤. ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.", []
        elif "insufficient_quota" in error_msg:
            return False, "âš ï¸ API í‚¤ëŠ” ìœ íš¨í•˜ì§€ë§Œ í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.", []
        else:
            return False, f"âŒ API í‚¤ ê²€ì¦ ì‹¤íŒ¨: {error_msg}", []


# ===== ë²¡í„° DB ì—…ë¡œë“œ ë° ê²€ì¦ í•¨ìˆ˜ =====
def extract_and_validate_vectordb(uploaded_file) -> tuple:
    """
    ì—…ë¡œë“œëœ ZIP íŒŒì¼ì„ ì¶”ì¶œí•˜ê³  ChromaDB êµ¬ì¡° ê²€ì¦
    
    Args:
        uploaded_file: Streamlit UploadedFile ê°ì²´
    
    Returns:
        (success, db_path, error_message)
    """
    import zipfile
    import tempfile
    import shutil
    
    try:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = tempfile.mkdtemp(prefix="chroma_db_")
        
        # ZIP íŒŒì¼ ì €ì¥
        zip_path = os.path.join(temp_dir, "chroma_db.zip")
        with open(zip_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # ZIP ì••ì¶• í•´ì œ
        extract_dir = os.path.join(temp_dir, "chroma_db")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        
        # ChromaDB í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = ["chroma.sqlite3"]
        
        # chroma_db í´ë” ë‚´ë¶€ í™•ì¸
        db_path = extract_dir
        
        # chroma.sqlite3 íŒŒì¼ ì°¾ê¸°
        found_sqlite = False
        for root, dirs, files in os.walk(extract_dir):
            if "chroma.sqlite3" in files:
                db_path = root
                found_sqlite = True
                break
        
        if not found_sqlite:
            shutil.rmtree(temp_dir)
            return False, None, "âŒ chroma.sqlite3 íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDB í´ë”ë¥¼ ì••ì¶•í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        
        # ì¶”ê°€ ê²€ì¦: íŒŒì¼ í¬ê¸° í™•ì¸
        sqlite_path = os.path.join(db_path, "chroma.sqlite3")
        file_size = os.path.getsize(sqlite_path)
        
        if file_size < 1024:  # 1KB ë¯¸ë§Œì´ë©´ ë¹„ì •ìƒ
            shutil.rmtree(temp_dir)
            return False, None, "âŒ ChromaDB íŒŒì¼ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì‘ìŠµë‹ˆë‹¤."
        
        return True, db_path, ""
        
    except zipfile.BadZipFile:
        return False, None, "âŒ ì˜ëª»ëœ ZIP íŒŒì¼ì…ë‹ˆë‹¤."
    except Exception as e:
        return False, None, f"âŒ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {str(e)}"


def get_vectordb_info(db_path: str) -> dict:
    """
    ë²¡í„° DB ì •ë³´ ì¡°íšŒ
    
    Args:
        db_path: ChromaDB ê²½ë¡œ
    
    Returns:
        ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    try:
        from langchain_chroma import Chroma
        from langchain_openai.embeddings import OpenAIEmbeddings
        
        # ì„ë² ë”© ì´ˆê¸°í™” (ì„ì‹œ)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        vectorstore = Chroma(
            persist_directory=db_path,
            embedding_function=embeddings
        )
        
        # ë¬¸ì„œ ìˆ˜ ì¡°íšŒ
        collection = vectorstore._collection
        doc_count = collection.count()
        
        # ìƒ˜í”Œ ë¬¸ì„œ ì¡°íšŒ
        sample_docs = vectorstore.get(limit=1)
        
        metadata_keys = []
        if sample_docs and sample_docs.get('metadatas') and len(sample_docs['metadatas']) > 0:
            metadata_keys = list(sample_docs['metadatas'][0].keys())
        
        return {
            'doc_count': doc_count,
            'metadata_keys': metadata_keys,
            'collection_name': collection.name
        }
        
    except Exception as e:
        return {
            'doc_count': 0,
            'metadata_keys': [],
            'error': str(e)
        }


# ===== RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” =====
@st.cache_resource
def initialize_rag(model_type, _user_api_key=None, gpt_model_name=None, custom_db_path=None):
    """
    RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    
    Args:
        model_type: "API ëª¨ë¸ (GPT)" ë˜ëŠ” "ë¡œì»¬ ëª¨ë¸ (GGUF)"
        _user_api_key: ì‚¬ìš©ìê°€ ì…ë ¥í•œ API í‚¤ (Noneì´ë©´ .env ì‚¬ìš©)
        gpt_model_name: ì‚¬ìš©í•  GPT ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gpt-4o-mini")
        custom_db_path: ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ë²¡í„° DB ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ)
    
    Returns:
        (rag_pipeline, error_message, model_name)
    """
    try:
        config = RAGConfig()
        
        # ì‚¬ìš©ì API í‚¤ê°€ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°
        if _user_api_key:
            config.OPENAI_API_KEY = _user_api_key
            os.environ["OPENAI_API_KEY"] = _user_api_key
        
        # GPT ëª¨ë¸ ì´ë¦„ ì„¤ì •
        if gpt_model_name:
            config.LLM_MODEL_NAME = gpt_model_name
        
        # ì»¤ìŠ¤í…€ ë²¡í„° DB ê²½ë¡œ ì„¤ì •
        if custom_db_path:
            config.DB_DIRECTORY = custom_db_path
        
        if model_type == "API ëª¨ë¸ (GPT)":
            # API ëª¨ë¸ ì‚¬ìš©
            from src.generator.generator import RAGPipeline
            rag = RAGPipeline(config=config)
            return rag, None, f"OpenAI {config.LLM_MODEL_NAME}"
            
        elif model_type == "ë¡œì»¬ ëª¨ë¸ (GGUF)":
            # GGUF ëª¨ë¸ ì‚¬ìš©
            from src.generator.generator_gguf import GGUFRAGPipeline
            
            rag = GGUFRAGPipeline(
                config=config,
                n_gpu_layers=35,
                n_ctx=8192,
                n_threads=4,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9
            )
            return rag, None, "Llama-3-Ko-8B (GGUF)"
        
        else:
            return None, f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {model_type}", None
            
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        return None, f"{str(e)}\n\n{error_detail}", None


# ===== ë‹µë³€ ìƒì„± =====
def generate_answer(query: str, top_k: int = 10, search_mode: str = "hybrid_rerank", alpha: float = 0.5):
    """ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
    try:
        result = st.session_state.rag_pipeline.generate_answer(
            query=query,
            top_k=top_k,
            search_mode=search_mode,
            alpha=alpha
        )
        return result
        
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        return {
            'answer': f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\n{error_detail}",
            'sources': [],
            'used_retrieval': False,
            'search_mode': search_mode,
            'routing_info': None,
            'usage': {'total_tokens': 0, 'prompt_tokens': 0, 'completion_tokens': 0}
        }


# ===== ë©”ì‹œì§€ í‘œì‹œ =====
def display_message(
    role: str, 
    content: str, 
    sources: list = None, 
    usage: dict = None, 
    search_mode: str = None,
    used_retrieval: bool = None,
    routing_info: dict = None
):
    """ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œ"""
    if role == 'user':
        st.markdown(f"""
        <div class="chat-message user-message">
            <div class="message-header">
                ğŸ‘¤ ì‚¬ìš©ì
            </div>
            <div class="message-content">
                {content}
            </div>
        </div>
        """, unsafe_allow_html=True)
        
    else:  # assistant
        # ë‹µë³€
        st.markdown(f"""
        <div class="chat-message assistant-message">
            <div class="message-header">
                ğŸ¤– ì±—ë´‡
            </div>
            <div class="message-content">
                {content}
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # ë¼ìš°íŒ… ì •ë³´ (ê°œë°œ ëª¨ë“œ)
        if st.session_state.show_routing_info and routing_info:
            route_icon = "ğŸ”" if routing_info.get('route') == 'rag' else "ğŸ’¬"
            st.markdown(f"""
            <div class="routing-info">
                {route_icon} ë¼ìš°íŒ…: {routing_info.get('route', 'N/A').upper()} 
                (ì‹ ë¢°ë„: {routing_info.get('confidence', 0):.2f}) - 
                {routing_info.get('reason', 'N/A')}
            </div>
            """, unsafe_allow_html=True)
        
        # ê²€ìƒ‰ ëª¨ë“œ ì •ë³´ (ê²€ìƒ‰ ì‚¬ìš© ì‹œë§Œ)
        if used_retrieval and search_mode:
            mode_display = {
                'hybrid_rerank': 'ğŸ”„ Hybrid + Re-ranker',
                'hybrid': 'ğŸ”€ Hybrid Search',
                'embedding_rerank': 'ğŸ“Š ì„ë² ë”© + Re-ranker',
                'embedding': 'ğŸ“Š ì„ë² ë”© ê²€ìƒ‰',
                'direct': 'ğŸ’¬ Direct (ê²€ìƒ‰ ì—†ìŒ)'
            }
            st.markdown(f"""
            <div class="search-mode-info">
                ê²€ìƒ‰ ëª¨ë“œ: {mode_display.get(search_mode, search_mode)}
            </div>
            """, unsafe_allow_html=True)
        
        # ì°¸ê³  ë¬¸ì„œ (ê²€ìƒ‰ ì‚¬ìš© ì‹œë§Œ)
        if used_retrieval and sources and len(sources) > 0:
            st.markdown("### ğŸ“š ì°¸ê³  ë¬¸ì„œ")
            
            for i, source in enumerate(sources, 1):
                metadata = source.get('metadata', {})
                
                # ê´€ë ¨ë„ ì ìˆ˜
                score = source.get('score', 0)
                score_type = source.get('score_type', '')
                
                # ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                content_preview = source.get('content', '')[:200] + "..."
                
                st.markdown(f"""
                <div class="source-document">
                    <div class="source-header">
                        ğŸ“„ ë¬¸ì„œ {i} (ì ìˆ˜: {score:.3f} / {score_type})
                    </div>
                    <div>
                        {content_preview}
                    </div>
                    <div class="metadata">
                        ğŸ“ íŒŒì¼: {metadata.get('íŒŒì¼ëª…', 'N/A')}<br>
                        ğŸ¢ ë°œì£¼ê¸°ê´€: {metadata.get('ë°œì£¼ ê¸°ê´€', 'N/A')}<br>
                        ğŸ“‹ ì‚¬ì—…ëª…: {metadata.get('ì‚¬ì—…ëª…', 'N/A')}
                    </div>
                </div>
                """, unsafe_allow_html=True)
        elif not used_retrieval:
            # ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²½ìš° ì•ˆë‚´
            st.info("ğŸ’¬ ì´ ë‹µë³€ì€ ë¬¸ì„œ ê²€ìƒ‰ ì—†ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # í† í° ì‚¬ìš©ëŸ‰
        if usage:
            st.markdown(f"""
            <div class="token-usage">
                ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {usage.get('total_tokens', 0)} 
                (í”„ë¡¬í”„íŠ¸: {usage.get('prompt_tokens', 0)}, 
                 ì™„ì„±: {usage.get('completion_tokens', 0)})
            </div>
            """, unsafe_allow_html=True)


# ===== ë©”ì¸ ì•± =====
def main():
    # í—¤ë”
    st.markdown('<div class="main-header">ğŸ¤– ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œ ì±—ë´‡</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">Query Router + RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ</div>', unsafe_allow_html=True)
    
    # ===== ì‚¬ì´ë“œë°” =====
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ===== ğŸ”‘ API í‚¤ ì„¤ì • =====
        st.markdown("### ğŸ”‘ API í‚¤ ì„¤ì •")
        
        config = RAGConfig()
        has_env_key = bool(config.OPENAI_API_KEY and config.OPENAI_API_KEY != "")
        
        if has_env_key:
            st.success("âœ… ì„œë²„ API í‚¤ ì‚¬ìš© ì¤‘")
        else:
            st.warning("âš ï¸ ì„œë²„ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì…ë ¥í•˜ì„¸ìš”.")
        
        use_custom_key = st.checkbox(
            "ğŸ”“ ë‚´ API í‚¤ ì‚¬ìš©í•˜ê¸°",
            value=not has_env_key,
            help="OpenAI API í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        
        if use_custom_key:
            user_key_input = st.text_input(
                "OpenAI API í‚¤ ì…ë ¥",
                type="password",
                placeholder="sk-...",
                help="https://platform.openai.com/api-keys ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                validate_button = st.button(
                    "ğŸ” ê²€ì¦",
                    use_container_width=True,
                    disabled=not user_key_input
                )
            
            with col2:
                apply_button = st.button(
                    "âœ… ì ìš©",
                    use_container_width=True,
                    disabled=not user_key_input,
                    type="primary"
                )
            
            # ê²€ì¦ ë²„íŠ¼
            if validate_button and user_key_input:
                with st.spinner("ğŸ”„ API í‚¤ ê²€ì¦ ë° ëª¨ë¸ ì¡°íšŒ ì¤‘..."):
                    is_valid, message, models = validate_api_key(user_key_input)
                    
                    if is_valid:
                        st.success(message)
                        st.session_state.api_key_validated = True
                        st.session_state.available_models = models
                        
                        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í‘œì‹œ
                        if models:
                            st.info(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(models)}")
                    else:
                        st.error(message)
                        st.session_state.api_key_validated = False
                        st.session_state.available_models = []
            
            # ì ìš© ë²„íŠ¼
            if apply_button and user_key_input:
                with st.spinner("ğŸ”„ API í‚¤ ì ìš© ì¤‘..."):
                    is_valid, message, models = validate_api_key(user_key_input)
                    
                    if is_valid:
                        st.session_state.user_api_key = user_key_input
                        st.session_state.api_key_validated = True
                        st.session_state.available_models = models
                        
                        # RAG íŒŒì´í”„ë¼ì¸ ì¬ì´ˆê¸°í™” ê°•ì œ
                        st.session_state.rag_pipeline = None
                        st.session_state.model_type = None
                        
                        st.success("âœ… API í‚¤ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        
                        if models:
                            st.info(f"ğŸ’¡ ì•„ë˜ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. ({len(models)}ê°œ ì‚¬ìš© ê°€ëŠ¥)")
                    else:
                        st.error(message)
            
            # API í‚¤ ì…ë ¥ ê°€ì´ë“œ
            with st.expander("ğŸ“– API í‚¤ ë°œê¸‰ ë°©ë²•"):
                st.markdown("""
                1. [OpenAI Platform](https://platform.openai.com/api-keys) ì ‘ì†
                2. ë¡œê·¸ì¸ í›„ "Create new secret key" í´ë¦­
                3. ìƒì„±ëœ í‚¤ë¥¼ ë³µì‚¬í•˜ì—¬ ìœ„ì— ë¶™ì—¬ë„£ê¸°
                
                **ì£¼ì˜ì‚¬í•­:**
                - API í‚¤ëŠ” ì•ˆì „í•˜ê²Œ ë³´ê´€í•˜ì„¸ìš”
                - ë¬´ë£Œ í¬ë ˆë”§ì´ ì†Œì§„ë˜ë©´ ì‚¬ìš© ë¶ˆê°€
                - ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ìš”ê¸ˆì´ ë¶€ê³¼ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤
                
                **ëª¨ë¸ë³„ ê°€ê²© (1M í† í° ê¸°ì¤€):**
                - gpt-4o: $2.50 (ì…ë ¥) / $10.00 (ì¶œë ¥)
                - gpt-4o-mini: $0.15 (ì…ë ¥) / $0.60 (ì¶œë ¥)
                - gpt-3.5-turbo: $0.50 (ì…ë ¥) / $1.50 (ì¶œë ¥)
                """)
        
        else:
            # ì„œë²„ í‚¤ ì‚¬ìš© ì¤‘
            if has_env_key:
                st.info("â„¹ï¸ ì„œë²„ì— ì„¤ì •ëœ API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                # ì„œë²„ í‚¤ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ (ìµœì´ˆ 1íšŒ)
                if not st.session_state.available_models:
                    with st.spinner("ğŸ”„ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ ì¤‘..."):
                        success, models, error = get_available_models(config.OPENAI_API_KEY)
                        if success:
                            st.session_state.available_models = models
            
            # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ ì´ˆê¸°í™”
            if st.session_state.user_api_key:
                st.session_state.user_api_key = None
                st.session_state.rag_pipeline = None
                st.session_state.model_type = None
        
        st.markdown("---")
        
        # ===== ğŸ“Š ë²¡í„° DB ì„¤ì • =====
        st.markdown("### ğŸ“Š ë²¡í„° DB ì„¤ì •")
        
        # í˜„ì¬ DB ìƒíƒœ í™•ì¸
        has_server_db = os.path.exists(config.DB_DIRECTORY)
        
        if has_server_db:
            st.success("âœ… ì„œë²„ ë²¡í„° DB ì‚¬ìš© ì¤‘")
        else:
            st.warning("âš ï¸ ì„œë²„ ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì—…ë¡œë“œí•˜ì„¸ìš”.")
        
        # ë²¡í„° DB ì—…ë¡œë“œ ì˜µì…˜
        use_custom_db = st.checkbox(
            "ğŸ“¤ ë‚´ ë²¡í„° DB ì—…ë¡œë“œí•˜ê¸°",
            value=not has_server_db,
            help="ìì‹ ì˜ ChromaDBë¥¼ ZIP íŒŒì¼ë¡œ ì—…ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        
        if use_custom_db:
            st.info("""
            ğŸ’¡ **ChromaDB ì¤€ë¹„ ë°©ë²•:**
            1. ChromaDB í´ë”ë¥¼ ZIPìœ¼ë¡œ ì••ì¶• (ì˜ˆ: `chroma_db.zip`)
            2. ZIP íŒŒì¼ ë‚´ë¶€ì— `chroma.sqlite3` íŒŒì¼ í¬í•¨ í•„ìˆ˜
            3. ì•„ë˜ì—ì„œ ì—…ë¡œë“œ
            """)
            
            uploaded_db = st.file_uploader(
                "ChromaDB ZIP íŒŒì¼ ì—…ë¡œë“œ",
                type=['zip'],
                help="chroma_db í´ë”ë¥¼ ì••ì¶•í•œ ZIP íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
            )
            
            if uploaded_db is not None:
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("ğŸ” ê²€ì¦", key="validate_db", use_container_width=True):
                        with st.spinner("ğŸ”„ ë²¡í„° DB ê²€ì¦ ì¤‘..."):
                            success, db_path, error = extract_and_validate_vectordb(uploaded_db)
                            
                            if success:
                                # DB ì •ë³´ ì¡°íšŒ
                                db_info = get_vectordb_info(db_path)
                                
                                if 'error' in db_info:
                                    st.error(f"âŒ DB ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {db_info['error']}")
                                else:
                                    st.success("âœ… ë²¡í„° DBê°€ ìœ íš¨í•©ë‹ˆë‹¤!")
                                    st.info(f"""
                                    ğŸ“‹ **DB ì •ë³´:**
                                    - ë¬¸ì„œ ìˆ˜: {db_info['doc_count']:,}ê°œ
                                    - ì»¬ë ‰ì…˜: {db_info['collection_name']}
                                    - ë©”íƒ€ë°ì´í„°: {', '.join(db_info['metadata_keys'][:5])}
                                    """)
                            else:
                                st.error(error)
                
                with col2:
                    if st.button("âœ… ì ìš©", key="apply_db", use_container_width=True, type="primary"):
                        with st.spinner("ğŸ”„ ë²¡í„° DB ì ìš© ì¤‘..."):
                            success, db_path, error = extract_and_validate_vectordb(uploaded_db)
                            
                            if success:
                                st.session_state.custom_db_path = db_path
                                st.session_state.db_uploaded = True
                                
                                # RAG íŒŒì´í”„ë¼ì¸ ì¬ì´ˆê¸°í™” ê°•ì œ
                                st.session_state.rag_pipeline = None
                                st.session_state.model_type = None
                                
                                st.success("âœ… ë²¡í„° DBê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                st.info("ğŸ’¡ ëª¨ë¸ì„ ë‹¤ì‹œ ì„ íƒí•˜ë©´ ìƒˆ ë²¡í„° DBë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")
                            else:
                                st.error(error)
            
            # ë²¡í„° DB ìƒì„± ê°€ì´ë“œ
            with st.expander("ğŸ“– ë²¡í„° DB ìƒì„± ë°©ë²•"):
                st.markdown("""
                **1. ë°ì´í„° ì¤€ë¹„**
                ```bash
                # ë¬¸ì„œ íŒŒì¼ì„ data/files/ í´ë”ì— ì €ì¥
                ```
                
                **2. ë²¡í„° DB ìƒì„±**
                ```bash
                # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                python main.py --step all
                
                # ë˜ëŠ” ì„ë² ë”©ë§Œ
                python main.py --step embed
                ```
                
                **3. ZIP ì••ì¶•**
                ```bash
                # Windows
                Compress-Archive -Path chroma_db -DestinationPath chroma_db.zip
                
                # Mac/Linux
                zip -r chroma_db.zip chroma_db/
                ```
                
                **4. ì—…ë¡œë“œ**
                - ìƒì„±ëœ `chroma_db.zip` íŒŒì¼ì„ ìœ„ì—ì„œ ì—…ë¡œë“œ
                
                **í•„ìˆ˜ íŒŒì¼:**
                - `chroma.sqlite3` (ë©”ì¸ DB íŒŒì¼)
                - `{uuid}/` í´ë”ë“¤ (ë²¡í„° ì¸ë±ìŠ¤)
                """)
        
        else:
            # ì„œë²„ DB ì‚¬ìš© ì¤‘
            if has_server_db:
                st.info("â„¹ï¸ ì„œë²„ì— ìˆëŠ” ë²¡í„° DBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                # ì„œë²„ DB ì •ë³´ í‘œì‹œ
                if st.button("ğŸ” DB ì •ë³´ ë³´ê¸°", key="view_server_db"):
                    with st.spinner("ğŸ”„ ì •ë³´ ì¡°íšŒ ì¤‘..."):
                        db_info = get_vectordb_info(config.DB_DIRECTORY)
                        
                        if 'error' in db_info:
                            st.error(f"âŒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {db_info['error']}")
                        else:
                            st.success(f"""
                            ğŸ“‹ **ì„œë²„ DB ì •ë³´:**
                            - ë¬¸ì„œ ìˆ˜: {db_info['doc_count']:,}ê°œ
                            - ì»¬ë ‰ì…˜: {db_info['collection_name']}
                            - ë©”íƒ€ë°ì´í„°: {', '.join(db_info['metadata_keys'][:5])}
                            """)
            
            # ì‚¬ìš©ì DB ì´ˆê¸°í™”
            if st.session_state.custom_db_path:
                st.session_state.custom_db_path = None
                st.session_state.db_uploaded = False
                st.session_state.rag_pipeline = None
                st.session_state.model_type = None
        
        st.markdown("---")
        
        # ===== ğŸ¤– ëª¨ë¸ ì„¤ì • =====
        st.markdown("### ğŸ¤– ëª¨ë¸ ì„¤ì •")
        
        can_use_gpt = has_env_key or (use_custom_key and st.session_state.api_key_validated)
        
        model_options = ["API ëª¨ë¸ (GPT)", "ë¡œì»¬ ëª¨ë¸ (GGUF)"]
        
        if not can_use_gpt:
            st.warning("âš ï¸ API í‚¤ë¥¼ ì…ë ¥í•´ì•¼ GPT ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            default_index = 1
        else:
            default_index = 0
        
        model_type = st.selectbox(
            "ìƒì„± ëª¨ë¸ ì„ íƒ",
            options=model_options,
            index=default_index,
            help="OpenAI API ë˜ëŠ” ë¡œì»¬ GGUF ëª¨ë¸ ì„ íƒ"
        )
        
        # ===== GPT ëª¨ë¸ ìƒì„¸ ì„ íƒ =====
        selected_gpt_model = None
        
        if model_type == "API ëª¨ë¸ (GPT)" and can_use_gpt:
            available_models = st.session_state.available_models
            
            if available_models:
                # ëª¨ë¸ ì„ íƒ UI
                st.markdown("#### ğŸ“‹ GPT ëª¨ë¸ ì„ íƒ")
                
                # ëª¨ë¸ ì„¤ëª…
                model_descriptions = {
                    'o3': 'ğŸŒŸ o3 ì‹œë¦¬ì¦ˆ (ìµœì²¨ë‹¨ ì¶”ë¡  ëª¨ë¸)',
                    'o3-mini': 'ğŸŒŸ o3-mini (ê²½ëŸ‰ ì¶”ë¡  ëª¨ë¸)',
                    'o1': 'ğŸ§  o1 ì‹œë¦¬ì¦ˆ (ê³ ê¸‰ ì¶”ë¡  ëª¨ë¸)',
                    'o1-mini': 'ğŸ§  o1-mini (ê²½ëŸ‰ ì¶”ë¡  ëª¨ë¸)',
                    'o1-preview': 'ğŸ§ª o1 í”„ë¦¬ë·° (ë² íƒ€)',
                    'gpt-5': 'âš¡ GPT-5 (ì°¨ì„¸ëŒ€ ëª¨ë¸)',
                    'gpt-5-turbo': 'âš¡ GPT-5 Turbo (ê³ ì†)',
                    'gpt-4o': 'ğŸš€ GPT-4o (ê°€ì¥ ê°•ë ¥)',
                    'gpt-4o-mini': 'âš¡ GPT-4o-mini (ë¹ ë¥´ê³  ì €ë ´, ê¶Œì¥)',
                    'gpt-4-turbo': 'ğŸ’ GPT-4 Turbo (ê³ ì„±ëŠ¥)',
                    'gpt-4': 'ğŸ† GPT-4 (ë†’ì€ í’ˆì§ˆ)',
                    'gpt-3.5-turbo': 'ğŸ’° GPT-3.5 Turbo (ê°€ì„±ë¹„)',
                    'gpt-3.5': 'ğŸ’° GPT-3.5 (ê¸°ë³¸)'
                }
                
                # format í•¨ìˆ˜: ëª¨ë¸ëª…ìœ¼ë¡œ ì„¤ëª… ì°¾ê¸°
                def get_model_display(model_name):
                    # ì •í™•íˆ ë§¤ì¹­ë˜ëŠ” ì„¤ëª… ì°¾ê¸°
                    if model_name in model_descriptions:
                        return f"{model_descriptions[model_name]} - {model_name}"
                    
                    # ë¶€ë¶„ ë§¤ì¹­ (ì˜ˆ: gpt-4o-2024-11-20 â†’ gpt-4o)
                    for key in model_descriptions.keys():
                        if model_name.startswith(key):
                            return f"{model_descriptions[key]} - {model_name}"
                    
                    # ë§¤ì¹­ ì•ˆë˜ë©´ ëª¨ë¸ëª…ë§Œ
                    return model_name
                
                # ê¸°ë³¸ê°’ ì„¤ì •
                if st.session_state.selected_gpt_model not in available_models:
                    # ìš°ì„ ìˆœìœ„: gpt-4o-mini > gpt-3.5-turbo > ì²«ë²ˆì§¸ ëª¨ë¸
                    if 'gpt-4o-mini' in available_models:
                        st.session_state.selected_gpt_model = 'gpt-4o-mini'
                    elif 'gpt-3.5-turbo' in available_models:
                        st.session_state.selected_gpt_model = 'gpt-3.5-turbo'
                    else:
                        st.session_state.selected_gpt_model = available_models[0]
                
                # ëª¨ë¸ ì„ íƒ
                selected_gpt_model = st.selectbox(
                    "ì‚¬ìš©í•  ëª¨ë¸",
                    options=available_models,
                    index=available_models.index(st.session_state.selected_gpt_model),
                    format_func=get_model_display,
                    help="API í‚¤ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ ì„ íƒí•˜ì„¸ìš”"
                )
                
                # ì„ íƒ ì €ì¥
                st.session_state.selected_gpt_model = selected_gpt_model
                
                # ì„ íƒí•œ ëª¨ë¸ ì •ë³´ í‘œì‹œ
                # ì„¤ëª… ì°¾ê¸°
                display_desc = "ì„¤ëª… ì—†ìŒ"
                for key, desc in model_descriptions.items():
                    if selected_gpt_model.startswith(key):
                        display_desc = desc
                        break
                
                st.markdown(f"""
                <div class="model-info">
                    ğŸ¯ <b>ì„ íƒëœ ëª¨ë¸</b><br>
                    â€¢ {selected_gpt_model}<br>
                    â€¢ {display_desc}
                </div>
                """, unsafe_allow_html=True)
                
            else:
                st.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì¡°íšŒí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                st.info("ğŸ’¡ 'ê²€ì¦' ë²„íŠ¼ì„ ëˆŒëŸ¬ ëª¨ë¸ ëª©ë¡ì„ ì¡°íšŒí•˜ì„¸ìš”.")
                
                # ê¸°ë³¸ê°’ ì‚¬ìš©
                selected_gpt_model = "gpt-4o-mini"
        
        elif model_type == "ë¡œì»¬ ëª¨ë¸ (GGUF)":
            # GGUF ëª¨ë¸ ì •ë³´ í‘œì‹œ
            st.markdown("""
            <div class="model-info">
                ğŸ–¥ï¸ <b>Llama-3-Ko-8B (GGUF)</b><br>
                â€¢ T4 GPU ê°€ì†<br>
                â€¢ ë¡œì»¬ ì‹¤í–‰ (ë¬´ë£Œ)<br>
                â€¢ ì´ˆê¸° ë¡œë”© ì‹œê°„ ì†Œìš”<br>
                â€¢ 35ê°œ ë ˆì´ì–´ GPU ì‚¬ìš©
            </div>
            """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # ===== ğŸ” ê²€ìƒ‰ ì„¤ì • =====
        st.markdown("### ğŸ” ê²€ìƒ‰ ì„¤ì •")
        
        search_mode = st.selectbox(
            "ê²€ìƒ‰ ëª¨ë“œ",
            options=["hybrid", "embedding"],
            index=0,
            format_func=lambda x: {
                "hybrid": "ğŸ”€ Hybrid Search (BM25 + ì„ë² ë”©)",
                "embedding": "ğŸ“Š ì„ë² ë”© ê²€ìƒ‰"
            }[x],
            help="Hybrid: í‚¤ì›Œë“œ + ì˜ë¯¸ ê²€ìƒ‰ ë³‘í–‰ (ê¶Œì¥)"
        )
        
        # Reranker í† ê¸€
        use_reranker = st.toggle(
            "ğŸ”„ Re-ranker ì‚¬ìš©",
            value=True,
            help="ê²€ìƒ‰ ê²°ê³¼ë¥¼ CrossEncoderë¡œ ì¬ì •ë ¬í•˜ì—¬ ì •í™•ë„ í–¥ìƒ (ê¶Œì¥)"
        )
        
        # ì‹¤ì œ ê²€ìƒ‰ ëª¨ë“œ ê²°ì •
        if use_reranker:
            if search_mode == "hybrid":
                actual_search_mode = "hybrid_rerank"
            else:  # embedding
                actual_search_mode = "embedding_rerank"
        else:
            actual_search_mode = search_mode
        
        top_k = st.slider(
            "ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (Top-K)",
            min_value=1,
            max_value=20,
            value=10,
            help="ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜"
        )
        
        alpha = st.slider(
            "ì„ë² ë”© ê°€ì¤‘ì¹˜ (alpha)",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.1,
            help="0: BM25ë§Œ, 1: ì„ë² ë”©ë§Œ, 0.5: ë™ì¼ ê°€ì¤‘ì¹˜ (Hybrid ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)",
            disabled=(search_mode == "embedding")
        )
        
        st.markdown("---")
        
        # ===== ğŸ› ï¸ ê°œë°œì ì˜µì…˜ =====
        st.markdown("### ğŸ› ï¸ ê°œë°œì ì˜µì…˜")
        
        show_routing = st.toggle(
            "ğŸ” ë¼ìš°íŒ… ì •ë³´ í‘œì‹œ",
            value=False,
            help="Routerì˜ íŒë‹¨ ê³¼ì •ì„ í‘œì‹œ (ë””ë²„ê¹…ìš©)"
        )
        st.session_state.show_routing_info = show_routing
        
        st.markdown("---")
        
        # ===== ğŸ’¬ ëŒ€í™” ê´€ë¦¬ =====
        st.markdown("### ğŸ’¬ ëŒ€í™” ê´€ë¦¬")
        
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.conv_manager.clear()
            st.rerun()
        
        if st.button("ğŸ’¾ ëŒ€í™” ë‹¤ìš´ë¡œë“œ", use_container_width=True):
            if len(st.session_state.conv_manager) > 0:
                json_str = st.session_state.conv_manager.export_to_json()
                
                st.download_button(
                    label="ğŸ“¥ JSON ë‹¤ìš´ë¡œë“œ",
                    data=json_str,
                    file_name=f"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json",
                    use_container_width=True
                )
        
        st.markdown("---")
        
        # ===== ğŸ“Š í†µê³„ =====
        st.markdown("### ğŸ“Š í†µê³„")
        stats = st.session_state.conv_manager.get_statistics()

        st.metric("ì´ ëŒ€í™” ìˆ˜", stats.get('total', 0))
        
        # í˜„ì¬ ì„¤ì • í‘œì‹œ
        st.markdown("---")
        st.markdown("### ğŸ“‹ í˜„ì¬ ì„¤ì •")
        st.text(f"ëª¨ë¸: {model_type}")
        if model_type == "API ëª¨ë¸ (GPT)" and selected_gpt_model:
            st.text(f"GPT ëª¨ë¸: {selected_gpt_model}")
        st.text(f"ê²€ìƒ‰ ëª¨ë“œ: {search_mode}")
        st.text(f"Re-ranker: {'âœ… ON' if use_reranker else 'âŒ OFF'}")
        st.text(f"ì‹¤ì œ ëª¨ë“œ: {actual_search_mode}")
        st.text(f"Top-K: {top_k}")
        if search_mode == "hybrid":
            st.text(f"Alpha: {alpha}")
        st.text(f"Router Info: {'âœ… ON' if show_routing else 'âŒ OFF'}")
    
    # ===== RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” =====
    # ëª¨ë¸ íƒ€ì…ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ GPT ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ íŒŒì´í”„ë¼ì¸ì´ ì—†ìœ¼ë©´ ì¬ì´ˆê¸°í™”
    need_reinit = (
        st.session_state.rag_pipeline is None or 
        st.session_state.model_type != model_type or
        (model_type == "API ëª¨ë¸ (GPT)" and 
         selected_gpt_model and 
         hasattr(st.session_state.rag_pipeline, 'model') and
         st.session_state.rag_pipeline.model != selected_gpt_model)
    )
    
    if need_reinit:
        with st.spinner(f"ğŸ”„ {model_type} ì´ˆê¸°í™” ì¤‘... (GGUF ëª¨ë¸ì€ 1~2ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            rag, error, rag_type = initialize_rag(
                model_type, 
                _user_api_key=st.session_state.user_api_key,
                gpt_model_name=selected_gpt_model,
                custom_db_path=st.session_state.custom_db_path
            )
            
            if error:
                st.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                
                with st.expander("ğŸ” ì—ëŸ¬ ìƒì„¸ ì •ë³´"):
                    st.code(error)
                
                st.info("""
                ### ğŸ’¡ í•´ê²° ë°©ë²•
                
                **GGUF ëª¨ë¸ ì‹¤íŒ¨ ì‹œ:**
                1. llama-cpp-python ì„¤ì¹˜ í™•ì¸:
```bash
pip install llama-cpp-python
```
                
                2. GGUF ëª¨ë¸ íŒŒì¼ í™•ì¸:
                   - config.yamlì˜ GGUF_MODEL_PATH ë˜ëŠ”
                   - MODEL_HUB_REPO ì„¤ì • í™•ì¸
                
                3. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ:
                   - n_gpu_layers ê°’ ê°ì†Œ (35 â†’ 20)
                
                **API ëª¨ë¸ ì‹¤íŒ¨ ì‹œ:**
                1. ChromaDBê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸:
```bash
python main.py --step embed
```
                
                2. OpenAI API í‚¤ í™•ì¸:
```bash
# .env íŒŒì¼
OPENAI_API_KEY=your-key-here
```
                
                3. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:
```bash
pip install rank-bm25 sentence-transformers
```
                """)
                return
            
            st.session_state.rag_pipeline = rag
            st.session_state.model_type = model_type
            
            # API í‚¤ ë° ëª¨ë¸ ì‚¬ìš© ì •ë³´ í‘œì‹œ
            if st.session_state.user_api_key:
                st.success(f"âœ… {rag_type} ì¤€ë¹„ ì™„ë£Œ! (ì‚¬ìš©ì API í‚¤)")
            else:
                st.success(f"âœ… {rag_type} ì¤€ë¹„ ì™„ë£Œ!")
    
    # ===== ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ =====
    st.markdown("---")
    
    if len(st.session_state.conv_manager) == 0:
        st.info("""
        ### ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!
        
        ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.
        
        **ì˜ˆì‹œ ì§ˆë¬¸:**
        - "ì•ˆë…•í•˜ì„¸ìš”" (ê²€ìƒ‰ ì•ˆ í•¨)
        - "ë°ì´í„° í‘œì¤€í™” ìš”êµ¬ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?" (ê²€ìƒ‰ ìˆ˜í–‰)
        - "ë³´ì•ˆ ê´€ë ¨ ìš”êµ¬ì‚¬í•­ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”" (ê²€ìƒ‰ ìˆ˜í–‰)
        - "ê³ ë§ˆì›Œìš”" (ê²€ìƒ‰ ì•ˆ í•¨)
        """)
    
    # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
    for msg in st.session_state.conv_manager.get_ui_history():
        display_message(
            role=msg['role'],
            content=msg['content'],
            sources=msg.get('sources'),
            usage=msg.get('usage'),
            search_mode=msg.get('search_mode'),
            used_retrieval=msg.get('used_retrieval'),
            routing_info=msg.get('routing_info')
        )
    
    # ===== ì§ˆë¬¸ ì…ë ¥ =====
    st.markdown("---")
    
    with st.form(key='question_form', clear_on_submit=True):
        user_input = st.text_area(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            height=100,
            placeholder="ì˜ˆ: ë°ì´í„° í‘œì¤€í™” ìš”êµ¬ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        )
        
        col1, col2, col3 = st.columns([1, 1, 4])
        
        with col1:
            submit_button = st.form_submit_button("ğŸ“¤ ì „ì†¡", use_container_width=True)
    
    # ===== ì§ˆë¬¸ ì²˜ë¦¬ =====
    if submit_button and user_input:

        # ë‹µë³€ ìƒì„±
        with st.spinner("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘..."):
            result = generate_answer(
                query=user_input,
                top_k=top_k,
                search_mode=actual_search_mode,
                alpha=alpha
            )
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.conv_manager.add_message(
            user_msg=user_input,
            ai_msg=result['answer'],
            query_type=result.get('query_type', 'unknown'),
            sources=result.get('sources', []),
            usage=result.get('usage', {}),
            search_mode=result.get('search_mode'),
            used_retrieval=result.get('used_retrieval', False),
            routing_info=result.get('routing_info')
        )
        
        # í™”ë©´ ìƒˆë¡œê³ ì¹¨
        st.rerun()


if __name__ == "__main__":
    main()