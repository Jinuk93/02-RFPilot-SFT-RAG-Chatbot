"""
RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ í‰ê°€ ë„êµ¬
- LangSmith Experiment ì‹¤í–‰
- Context Precision/Recall í‰ê°€
- ì‹¤í—˜ ì¶”ì  ë° ë¹„êµ

ì‚¬ìš©ë²•:
    python run_experiment.py              # ëŒ€í™”í˜• ë©”ë‰´
    python run_experiment.py --run        # ì‹¤í—˜ ì‹¤í–‰
    python run_experiment.py --compare    # ì‹¤í—˜ ë¹„êµ
"""

import os
import re
import sys
import argparse
from pathlib import Path
from typing import Dict, List, Any
from langsmith import Client, evaluate
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

from src.retriever.retriever import RAGRetriever
from src.utils.config import RAGConfig
from src.evaluation.experiment_tracker import ExperimentTracker


# === í™˜ê²½ ì„¤ì • ===
load_dotenv()
os.environ["LANGCHAIN_PROJECT"] = "RAG-Retriever-Eval"
os.environ["LANGCHAIN_TRACING_V2"] = "true"


# === ì „ì—­ ë³€ìˆ˜ ===
retriever = None


# ============================================================
# Evaluator í•¨ìˆ˜ë“¤
# ============================================================

def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    # ì†Œë¬¸ì ë³€í™˜
    normalized = text.lower()
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    normalized = re.sub(r'[\r\n\t]+', ' ', normalized)
    
    # ì—°ì† ê³µë°± í•˜ë‚˜ë¡œ
    normalized = ' '.join(normalized.split())
    
    return normalized.strip()


def is_matching_context(retrieved_text: str, ground_truth_text: str, threshold: float = 0.5) -> bool:
    """ë‘ ë¬¸ì„œê°€ ê°™ì€ ë¬¸ì„œì¸ì§€ íŒë‹¨"""
    normalized_retrieved = normalize_text(retrieved_text)
    normalized_truth = normalize_text(ground_truth_text)
    
    # ì™„ì „ í¬í•¨ ì²´í¬
    if normalized_truth in normalized_retrieved:
        return True
    
    if normalized_retrieved in normalized_truth:
        return True
    
    # ë‹¨ì–´ ì»¤ë²„ë¦¬ì§€ ì²´í¬
    truth_words = set(normalized_truth.split())
    retrieved_words = set(normalized_retrieved.split())
    
    if len(truth_words) == 0:
        return False
    
    matched_words = truth_words & retrieved_words
    coverage = len(matched_words) / len(truth_words)
    
    return coverage >= threshold


def count_matching_contexts(
    retrieved_contexts: List[str],
    ground_truth_contexts: List[str],
    threshold: float = 0.5
) -> int:
    """ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œ ê°œìˆ˜ ê³„ì‚°"""
    matched_count = 0
    
    for retrieved in retrieved_contexts:
        for truth in ground_truth_contexts:
            if is_matching_context(retrieved, truth, threshold):
                matched_count += 1
                break
    
    return matched_count


def context_precision_evaluator(run: Any, example: Any) -> Dict[str, float]:
    """Context Precision í‰ê°€"""
    try:
        # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
        if isinstance(run.outputs, dict):
            retrieved_results = run.outputs.get('output', [])
        else:
            retrieved_results = run.outputs
        
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        retrieved_contexts = []
        for result in retrieved_results:
            if isinstance(result, dict):
                text = result.get('content', '')
                if text:
                    retrieved_contexts.append(text)
        
        # ì •ë‹µ ì¶”ì¶œ
        ground_truth_contexts = example.outputs.get('ground_truth_contexts', [])
        
        # ê²€ì¦
        if len(retrieved_contexts) == 0:
            return {"key": "context_precision", "score": 0.0, "comment": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"}
        
        if len(ground_truth_contexts) == 0:
            return {"key": "context_precision", "score": 0.0, "comment": "ì •ë‹µ ì—†ìŒ"}
        
        # ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚°
        matched_count = count_matching_contexts(
            retrieved_contexts,
            ground_truth_contexts,
            threshold=0.5
        )
        
        # Precision ê³„ì‚°
        precision = matched_count / len(retrieved_contexts)
        
        return {
            "key": "context_precision",
            "score": precision,
            "comment": f"ë§¤ì¹­: {matched_count}/{len(retrieved_contexts)}"
        }
        
    except Exception as e:
        print(f"Context Precision ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {"key": "context_precision", "score": 0.0, "comment": f"ì˜¤ë¥˜: {str(e)}"}


def context_recall_evaluator(run: Any, example: Any) -> Dict[str, float]:
    """Context Recall í‰ê°€"""
    try:
        # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
        if isinstance(run.outputs, dict):
            retrieved_results = run.outputs.get('output', [])
        else:
            retrieved_results = run.outputs
        
        retrieved_contexts = []
        for result in retrieved_results:
            if isinstance(result, dict):
                text = result.get('content', '')
                if text:
                    retrieved_contexts.append(text)
        
        # ì •ë‹µ ì¶”ì¶œ
        ground_truth_contexts = example.outputs.get('ground_truth_contexts', [])
        
        # ê²€ì¦
        if len(ground_truth_contexts) == 0:
            return {"key": "context_recall", "score": 0.0, "comment": "ì •ë‹µ ì—†ìŒ"}
        
        if len(retrieved_contexts) == 0:
            return {"key": "context_recall", "score": 0.0, "comment": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"}
        
        # ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚°
        matched_count = 0
        for truth in ground_truth_contexts:
            for retrieved in retrieved_contexts:
                if is_matching_context(retrieved, truth, threshold=0.5):
                    matched_count += 1
                    break
        
        # Recall ê³„ì‚°
        recall = matched_count / len(ground_truth_contexts)
        
        return {
            "key": "context_recall",
            "score": recall,
            "comment": f"ë°œê²¬: {matched_count}/{len(ground_truth_contexts)}"
        }
        
    except Exception as e:
        print(f"Context Recall ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {"key": "context_recall", "score": 0.0, "comment": f"ì˜¤ë¥˜: {str(e)}"}


def retrieval_time_evaluator(run: Any, example: Any) -> Dict[str, float]:
    """ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •"""
    try:
        latency = run.execution_time
        return {
            "key": "retrieval_time",
            "score": latency,
            "comment": f"{latency:.3f}ì´ˆ"
        }
    except Exception as e:
        return {"key": "retrieval_time", "score": 0.0, "comment": "ì‹œê°„ ì¸¡ì • ì‹¤íŒ¨"}


# ============================================================
# Target í•¨ìˆ˜
# ============================================================

def retriever_target(inputs: dict) -> dict:
    """LangSmith Experimentìš© ê²€ìƒ‰ í•¨ìˆ˜"""
    question = inputs.get("question", "")
    
    if not question:
        return {"output": []}
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + Re-ranker ì‹¤í–‰
    results = retriever.search_with_mode(
        query=question, 
        top_k=None, 
        mode="hybrid_rerank", 
        alpha=0.5
    )
    
    return {"output": results}


# ============================================================
# ì‹¤í—˜ ì‹¤í–‰
# ============================================================

def run_experiment(
    experiment_name: str,
    config: dict,
    dataset_name: str = "RAG-Retriever-TestSet-v1",
    notes: str = ""
) -> dict:
    """
    ì‹¤í—˜ ì‹¤í–‰ ë° ìë™ ì¶”ì 
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        config: ì‹¤í—˜ ì„¤ì •
        dataset_name: Dataset ì´ë¦„
        notes: ë©”ëª¨
        
    Returns:
        ì‹¤í—˜ ê²°ê³¼
    """
    global retriever
    
    print("\n" + "="*80)
    print(f"ğŸš€ ì‹¤í—˜ ì‹œì‘: {experiment_name}")
    print("="*80)
    
    # 1. ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    print("\nğŸ”§ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”...")
    rag_config = RAGConfig()
    
    # Config ì ìš©
    if 'embedding_model' in config:
        rag_config.EMBEDDING_MODEL_NAME = config['embedding_model']
    if 'top_k' in config:
        rag_config.DEFAULT_TOP_K = config['top_k']
    
    retriever = RAGRetriever(config=rag_config)
    
    print(f"âœ… ì„¤ì • ì™„ë£Œ:")
    print(f"   ì„ë² ë”© ëª¨ë¸: {rag_config.EMBEDDING_MODEL_NAME}")
    print(f"   Top-K: {rag_config.DEFAULT_TOP_K}")
    
    # 2. Evaluators ì„¤ì •
    evaluators_list = [
        context_precision_evaluator,
        context_recall_evaluator,
    ]
    
    # 3. LangSmith Client ì´ˆê¸°í™”
    client = Client()
    
    # 4. Experiment ì‹¤í–‰
    print(f"\nâ³ Experiment ì‹¤í–‰ ì¤‘...")
    
    try:
        results = evaluate(
            retriever_target,
            data=dataset_name,
            evaluators=evaluators_list,
            experiment_prefix=experiment_name,
            max_concurrency=1,
        )
        
        print(f"\nâœ… Experiment ì™„ë£Œ!")
        
        # 5. ê²°ê³¼ ì¶”ì¶œ
        df = results.to_pandas()
        
        metrics = {
            "precision": df["feedback.context_precision"].mean(),
            "recall": df["feedback.context_recall"].mean(),
            "avg_time": df["execution_time"].mean(),
        }
        
        # 6. ìë™ ì¶”ì  ì €ì¥
        tracker = ExperimentTracker()
        
        langsmith_url = "https://smith.langchain.com/"
        
        tracker.log_experiment(
            experiment_name=experiment_name,
            config=config,
            metrics=metrics,
            langsmith_url=langsmith_url,
            notes=notes
        )
        
        # 7. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š ì‹¤í—˜ ê²°ê³¼")
        print("="*80)
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall: {metrics['recall']:.4f}")
        
        f1 = 0
        if (metrics['precision'] + metrics['recall']) > 0:
            f1 = 2 * metrics['precision'] * metrics['recall'] / (metrics['precision'] + metrics['recall'])
        print(f"F1: {f1:.4f}")
        print(f"í‰ê·  ê²€ìƒ‰ ì‹œê°„: {metrics['avg_time']:.3f}ì´ˆ")
        print("="*80)
        
        return results
        
    except Exception as e:
        print(f"\nâŒ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise


# ============================================================
# ëŒ€í™”í˜• ë©”ë‰´
# ============================================================

def interactive_run():
    """ëŒ€í™”í˜• ì‹¤í—˜ ì‹¤í–‰"""
    print("\n" + "="*80)
    print("ğŸ§ª RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì‹¤í—˜")
    print("="*80)
    
    # ì‹¤í—˜ ì„¤ì • ì…ë ¥
    print("\nì‹¤í—˜ ì„¤ì •ì„ ì…ë ¥í•˜ì„¸ìš”:")
    
    experiment_name = input("ì‹¤í—˜ ì´ë¦„ (ì˜ˆ: baseline, hybrid-rerank): ").strip()
    if not experiment_name:
        experiment_name = "experiment"
    
    embedding_model = input("ì„ë² ë”© ëª¨ë¸ (ì—”í„°: text-embedding-3-small): ").strip()
    if not embedding_model:
        embedding_model = "text-embedding-3-small"
    
    top_k_input = input("Top-K (ì—”í„°: 10): ").strip()
    top_k = int(top_k_input) if top_k_input else 10
    
    notes = input("ë©”ëª¨ (ì„ íƒì‚¬í•­): ").strip()
    
    # ì„¤ì • êµ¬ì„±
    config = {
        "embedding_model": embedding_model,
        "top_k": top_k,
    }
    
    # í™•ì¸
    print("\n" + "="*80)
    print("ğŸ“‹ ì‹¤í—˜ ì •ë³´ í™•ì¸")
    print("="*80)
    print(f"ì‹¤í—˜ ì´ë¦„: {experiment_name}")
    print(f"ì„ë² ë”© ëª¨ë¸: {embedding_model}")
    print(f"Top-K: {top_k}")
    if notes:
        print(f"ë©”ëª¨: {notes}")
    print("="*80)
    
    confirm = input("\nì‹¤í—˜ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
    if confirm != 'y':
        print("âŒ ì·¨ì†Œë¨")
        return
    
    # ì‹¤í—˜ ì‹¤í–‰
    run_experiment(
        experiment_name=experiment_name,
        config=config,
        notes=notes
    )


def interactive_compare():
    """ëŒ€í™”í˜• ì‹¤í—˜ ë¹„êµ"""
    tracker = ExperimentTracker()
    
    print("\n" + "="*80)
    print("ğŸ” ì‹¤í—˜ ë¹„êµ ë„êµ¬")
    print("="*80)
    
    while True:
        print("\në©”ë‰´:")
        print("  1. ëª¨ë“  ì‹¤í—˜ ëª©ë¡ ë³´ê¸°")
        print("  2. ìµœê·¼ ì‹¤í—˜ ë¹„êµ (ìµœê·¼ 5ê°œ)")
        print("  3. íŠ¹ì • ì‹¤í—˜ ë¹„êµ")
        print("  4. ê°œì„  íš¨ê³¼ í™•ì¸")
        print("  5. ì°¨íŠ¸ ìƒì„±")
        print("  6. ìµœì  ì„¤ì • ì¶”ì²œ")
        print("  0. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ: ").strip()
        
        if choice == "1":
            tracker.list_experiments()
        
        elif choice == "2":
            tracker.compare_experiments(top_n=5)
        
        elif choice == "3":
            names = input("ì‹¤í—˜ ì´ë¦„ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„): ").strip()
            if names:
                experiment_names = [n.strip() for n in names.split(',')]
                tracker.compare_experiments(experiment_names=experiment_names)
        
        elif choice == "4":
            baseline = input("Baseline ì‹¤í—˜ ì´ë¦„: ").strip()
            current = input("ë¹„êµí•  ì‹¤í—˜ ì´ë¦„: ").strip()
            
            if baseline and current:
                tracker.show_improvement(baseline, current)
        
        elif choice == "5":
            names_input = input("ì‹¤í—˜ ì´ë¦„ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì—”í„°: ì „ì²´): ").strip()
            
            if names_input:
                experiment_names = [n.strip() for n in names_input.split(',')]
            else:
                experiment_names = None
            
            tracker.plot_metrics(experiment_names=experiment_names)
        
        elif choice == "6":
            metric = input("ê¸°ì¤€ ì§€í‘œ (precision/recall/f1, ì—”í„°: f1): ").strip()
            if not metric:
                metric = "f1"
            
            tracker.recommend_best(metric=metric)
        
        elif choice == "0":
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤")
            break
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤")


def main_menu():
    """ë©”ì¸ ë©”ë‰´"""
    print("\n" + "="*80)
    print("ğŸ”¬ RAG í‰ê°€ ì‹œìŠ¤í…œ")
    print("="*80)
    
    while True:
        print("\në©”ë‰´:")
        print("  1. ì‹¤í—˜ ì‹¤í–‰")
        print("  2. ì‹¤í—˜ ë¹„êµ")
        print("  0. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ: ").strip()
        
        if choice == "1":
            interactive_run()
        
        elif choice == "2":
            interactive_compare()
        
        elif choice == "0":
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤")
            break
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤")


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(description='RAG í‰ê°€ ì‹œìŠ¤í…œ')
    
    parser.add_argument(
        '--run',
        action='store_true',
        help='ì‹¤í—˜ ì‹¤í–‰ ëª¨ë“œ'
    )
    
    parser.add_argument(
        '--compare',
        action='store_true',
        help='ì‹¤í—˜ ë¹„êµ ëª¨ë“œ'
    )
    
    args = parser.parse_args()
    
    try:
        if args.run:
            interactive_run()
        elif args.compare:
            interactive_compare()
        else:
            main_menu()
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()